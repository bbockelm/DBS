**********************************************************************
* Baseline system

The baseline system documented in this chapter is based on our
current understanding of how to provide a system with demonstrated
capability to support the expected CMS data processing needs for
the two first years following LHC turn-on in 2007, and demonstrated
ability to evolve to meet new goals.

The schedule chapter describes how the required functionality and
evolution properties will be demonstrated.  In short, the baseline
system will be evolved and refactored from the present
services to the desired system, continually maintaining a fully
operational system.  Development is focused on integration testbed
milestones approximately every three months.  A prescribed level of
functionality, scalability and convenience of operation and
maintenance must be demonstrated at these testbeds.  In between the
milestones smaller improvements to the operational systems will be
made.  Passing the milestone indicates both readiness to handle a new
level of CMS data processing needs, and ability to evolve the systems
to meet new goals. [FIXME: project checkpoint] [FIXME:
backward/forward compatibility.]

[FIXME: integration testbeds, challenges, certification, see
scheduling chapter.]

The rationale for the baseline system is described in the previous
chapter.  In brief, we intend to retain the data management system as
a fairly loosely coupled set of services and components where critical
communication is in general asynchronous and agent-based.

There is no fallback to the baseline solution as such.  Due to the
development model there will always be a working system that can be
used and developed to a different direction should it look like that
the data management project will be unable to deliver the desired
system in time.  Each service challenge milestone provides a project
checkpoint to determine if the development is on track and allows the
plans to be revisited.

The baseline system can be characterised as conservative; this is a
deliberate choice on our part.  New services and components may emerge
with desirable functionality richer than described in the baseline and
demonstrated sufficient robustness.  We anticipate to take advantage
of such components, while not risking the design of the baseline to
depend on them.

Scheduling, project organisation and other planning is described
in a separate chapter.

FIXME: need to specify what is free for site to choose

FIXME: CMS manages its own data in some large chunks.  CMS provides
the system that tracks where the data is, and makes decisions on
placements etc.  How to access the data is found out when jobs arrive
at the site.

** System overview

FIXME: System overview, deployment model, interactions.  How many
instances of what are running where, which systems, services or
components they interact with and how.

** User access

Data discovery, queries, web pages, command line tools.  PHYSH like
tools.

** Framework interaction

Expand on what framework provides and receives.

Environment for which we used to build tools no longer exists (private
farms, basic bookkeeping etc.).  Will exist even less after new EDM.
Prune the old stuff out from both framework, production tools, overall
in the system.  Work out how parameters from data management /
production assignments etc. get provided to the jobs, get information
back out the other way.  In general moving much of this under DM
tools, less so in WM/production tools.

Running entire chains of jobs (cmkin, oscar, orca, dst, ntuple).


*** Dataset bookkeeping system

*** Data Organization

FIXME: Brief description on the fundamental assumptions on data
organization: event model, vertical streams and production datasets,
mapping of events to files, etc

*** Datasets

FIXME: Dataset definition, relationships, virtual data, dataset
creation, etc.

Datasets consist of a collection of events of a certain type (as
detemined by physics meta-data, like their trigger path or physics
generator). Datasets become streams of eventdata for processing
(input/output) by specifying an event range (by giving a *constraint*)
and a event representation/processing step (?) (by specifying a
*processing ID*), see below.

Physics users thus query the physics metadata to pick and discover
datasets of their choosing, and they investigate physics metadata
available for a data sets.

Physics Metadata relates to a dataset or a subset of events from a
dataset, and is distinct from other metadata, like the e.g. file-level
metadata etc.

The collection of events that belong to a dataset is not fixed, but
will change during data taking or when more events are being added for
simulated data sets.

To define a specific representation of a specific collection of CMS
events, like when specifying an RAW or AOD input stream for event
processing, one would need specify

-- a dataset identifier (DSID) 

-- a set of constraints (even range, run range, validity period, data
   quality requirements, integrated luminosity etc)

-- a parameterset identifier, which defines the processing that the
   event data in the event [FIXME needs work on how these entities get
   defined, see EDM]

Each dataset has a unique DSID. A physics user obtains a DSID by going
to the datset bookkeeping system DBS and querying the physics
metadata, or by resolving a production assignment ID etc.

FIXME: Purpose for queries

Describe role and structure of physics metadata as attributes of data
sets, that gets queried

FIXME: Run definition.

*** Bookkeeping system

Data atoms, unbreakable self-consistent unit of data (but consider
data hierarchy and dependencies), unique key.  Could be different for
data management and workload management (blocks vs. sets of runs) =>
two-way negotiation for best job split.  Compromise between user
desire of well-defined chunk and scalability.  Main issue: granularity
of data management vs. workload management vs. application itself.
User specification of input data dm and wm must refine into something
that is then passed to application -- what is the correct "chunk" --
LHC fill?  it becomes the set application needs to run on.

For WM data atoms are abstract and somehow lead to files to be
provided to the application to run on; wm doesn't know what files are
in an atom, dm does; atom itself is the smallest unit delivered to an
application.  Today: run / tier  + meta data => maps to N files.

virtual data catalogue
logical file catalogue
various other catalogues (runss, files, blocks)

not purely file-level -- different level of details costs different
amount.  some things are done at higher (3 or more orders of magnitude
more) level units like blocks; getting info below that level is not
available in dbs.

cvs-like tags.

open-closed datasets / blocks / whatever units

attribute tags -- data quality etc.  may apply to entire datasets as
well as smaller units, e.g. runs.

Logical file catalogue: size, checksum, conditions links?

Replica tracking philosophy -- at blocks level, site ownership of this
data, how it propagates back to central database (through agents?).
sites specify what data they serve, or specify what they have?

Queries

*** Linkage with other systems

Production: preparation, summary, file registration, dataset creation,
import datasets wholesale; providing dataset "transactions" like
"create new dataset" and bookkeeping of dataset-related metadata;
coupling to production processing (and eventually data taking) through
"importing" of files into a dataset; asynchronous update of
information.

Linkage with workload management; chunks are important in practice,
but should probably be treated as an optimization vehicle for data
placement?

Linkage with transfer -- blocks as replication unit

Linkage with other systems -- conditions, lumi information, etc.

Linkage with data quality tracking.

*** Operational behaviour

Asynchronous implementation through daemon/agent like things,
instead of active push/pull of information.


-------

FIXME: Not just file-level

Tracks datasets at various level of detail: trigger stream / offline
stream / <various params > / chunk...  Chunk is the smallest atom of
data moved around (but for mc production files may originally be
scattered -- but then source will probably be forgotten). Chunks may
be open or closed; once closed, immutable.  Files, chunks and entire
datasets can carry various attribute tags (including "bad").  Consider
here different slices of data: depends on how many files we write out,
and which ones are needed for input of different types of jobs.

Tracks replicas of chunks.  Detail greater than this only known in
data transfer system and at sites involved.  Chunk replica tracking
based on peer-to-peer network where each site manages and controls
knowledge of what chunks exist at that site, plus communicates that to
its "neighbours" or central point.

Knows which files belong to which dataset -- logical file catalogue --
and attributes of those files such as size, checksum, luminosity, etc.
Doesn't know about file replicas however, this is handled by chunk
replicas and site-local catalogues.

Asynchronously learns about new files, i.e. there's some agents that
pick up job outputs at various sites and propagate that knowledge to
dataset bookkeeping system.  Or some other means of asynchronous
harvesting -- rgma etc.  Able to handle virtual data (create new
dataset, then start filling it in) as well as data adoption (produce
something then decide to make it known; may be refused if parametres
weren't correct).  Data transfer operates asynchronously of these
processes.

FIXME: I don't particularly like the formulation "learns about" and
the agents pardigm for this one. I thought a production job could
"check in" or import a file to a dataset, which is a transaction of
the system, and from then on does not have to deal any more with the
specific file? If there is no such transaction how can the production
job finish and release storage etc?
----


FIXME: File merging. File size a function of limits on a single job.
Does merging retain file parentage information; when does it occur,
what are the constraints?  Q: What happens with chunks when files get
merged?  Impact on cross-file references (not allowed to merge after
references to files outside certain set have been made?)  Implications
on lost files -- merged related files should be constrained to be
synchronised in contents so these are easier to track?

FIXME: Data management system should account for what constitutes a
dataset.  The concepts and procedures should be simple enough that
handling datasets should be at least as simple as handling individual
files -- otherwise we might as well handle just files!

FIXME: Seen from operations and reported bugs: current work models are
complex and are to be simplified.  Constructing datasets error prone.
Reworking entire system, including framework, production tools, etc.,
not to enshrine current stuff as the data management system.
Proactively make sure end-to-end system makes sense, not building
complexity from the outside.

FIXME: Migration policy of current data to the new EDM (old data is
incompatible with the new EDM).  Similar issues will arise in future.
Accessing legacy systems for legacy data.  End-of-life policy.

FIXME: File naming conventions.  Especially if we use hierarchical LFN
space.  File merging consequences on file names.  Avoid putting too
much info in the name, names have to be of limited length.

FIXME: Blocks and data tiers.  Blocks of data for different tiers
should match, otherwise very hard to tell if the data available at a
site is really useful.  Or blocks should match runs?  But then, if all
data tiers are always in one file, this is moot.

FIXME: publishing system.  currently pubdb, advertise in refdb.  needs
to be re-divided between phedex (block tracking), pubdb, workload
management needs.


** Parametre system

Detail: My understanding is that event format like RAW or AOD is given
by the parameter ID?

FIXME: relationship to current owner information
FIXME: Also stores all parameters used by each algorithm?

    - evolution from rcp
    - which parameters can be queried?  which matter to the
      definition of dataset?  "what information is important?"
    - evolution of parameter attributes (queried -> not queried)
       - schema tied to orca version?
    - more important problems: managing dataset privately produced
      to publish, not super versatile query system
    - management through central database; flat file with job
    - interface area with dm / edm

** Conditions data

Interaction with...

CMS estimates of calibration data.  Very large (up to 5% of raw)?
Should calibration data management happen exactly the same as
normal data management?  E.g. stored in files, shipped around
using normal transfer mechanisms?

** Data transfer system

Much like PhEDEx is now.  Will move to peer-to-peer, with T0/T1s
possibly sharing a database.  Needs to be trivial to install and
operate/maintain, 0.0N FTE.

Push/pull/stream modes.  Pure data transfer system, not a replica
catalogue, not rest of "workflow".  Scales by # of files in transfer.
Leverages data chunking at higher level -- but doesn't require a chunk
to exist anywhere in entirety.  Robust, internet-like design, no one
part can bring entire transfer network down.  Maintains abstractions
at different layers.  Should be able to saturate any hardware
(network, storage) thrown at it.  Operates completely asynchronously
from other data management systems, production, worker nodes, ...

Layers/modules:
1. Unreliable point-to-point file transfer: globus-url-copy, srmcp, ...
2. Reliable point-to-point (= single-hop) transfer, local resource management
3. Reliable routed (= multi-hop) transfer
4. Dataset/chunk-level data transfer
4.a. Allocate files to destinations, back-up routing etc.
4.b. Monitor transfers at chunk level, notify site on progress
4.c .Activate/deactivate chunks
5. Placing files into transfer
5.a. Production link: harvest files from completed jobs
5.b. Bulk transfer requests: pick transfer assignments

Timing of SRM availability at all sites?

** Validation

Avoid over-validation.  Hierarchical validation (validating X also
validates Y.)

Technical validation.  As close to production as possible.  Make file
integrity guarantees (checksum, size, ...).  EDM (/POOL/Root) tool to
verify file integrity.  Produce a summary file, no need to validate
this information further until published.  Does not verify data
quality.  Event bit handling is in other files.

Transfer validation.  Ensures file integrity.

[Currently have site-local validation. This is to be suppressed if at
all possible.  If we get the same files over to the other side, we
should have everything correct by definition.  Need to think how this
interacts with publishing.  If meta files are no longer needed and
transfer systems idea of available blocks == current pubdb
information, then there's little more to do here.]

Physics validation, including calibration validations.  Adds data
quality tags to dataset bookkeeping system.  Data is not removed
at this point, it is only tagged.  People will simply not use
data without the tags they require.

[Tony/Nicola have much more detailed division.]

Handling lost files.  How many other files need to be marked lost --
if all data is in one file, only one file is affected by lost file.
Luminosity etc. sections should map to individual files.

Where validation results are recorded.  DBS validation tags.
Interaction with local vs. global DBS -- can I add validation tags in
my own local replica of the global DBS?

Where is what done?  DM tracks validation, need to define workflows.
Who takes care of doing this.  Eventually a data quality system?  Is
this a separate database?  Like luminosity/conditions tracking system?

 validation shouldn't be tied to unit of dataset, but incremental
   while files are being produced and stage in
   as light-weight as possible
   as early as possible

 validation must also be possible at any time (disks lost, ...)
   try to come up with finely grained list of all possible checks,
    when what why how cost, how to record results
    overlapping validations (e.g. hits validated by digitisation)
      -> implications for analysis?

 additional validation for skimming, file merging

 validating the data management itself of the files
   (when bookkeeping doesn't match files)

 model for first-pass reconstruction, including calibration validations

 when importing files, must not need to go through every file



** File catalogues and access authorization

Two separate aspects here: catalogue client and implementation.
Client is POOL interface.  Implementation may be local look-up table
(no service) or site-local file catalogue based on relational
database.  Auxiliary catalogues (XML) may be used, either accessed
directly from storage or downloaded to worker node with http/gsiftp
(external connectivity not required).  The catalogue used to record
and look up replica paths, nothing else.

The catalogue is to be entirely *local*, but may contain several
replicas for local load balancing (is this in conflict with previous
point of logical name spaces?).  Catalogues are updated and accessed
by the data transfer system (update a no-op for lookup table).  We do
not use catalogues for cross-site replica discovery.

FIXME: in major practical cases like the Castor or dCache based
storage elements that functionality is basically provided by the SE
"names space" (PNFS in case of dCache). These systems are already
highly optimized and robust (or at least equally robust as the storage
itself), so we probably should directly use them. One way of doing
that is to use URL-style file names everywhere, which also (as far as
I understand) provides access control etc.

FIXME: Attractive to try to suppress catalogues altogether.  Jobs
would discover prefix configuration probably using the same mechanism
they get their software / other site-specific information for the jobs
(obviously not a separate file on every worker node).

FIXME: access control lists, authorization.  Role-based security based
on groups?  Can ACLs be implemented robustly at all at timescale of
baseline?  [FIXME: Ian Fisk's comments on role-based mapping to GIDs
in UID.GID standard POSIX security model.  Dynamically created groups
a tough problem.  Currently implemented in globus gatekeeper and
dcache.  can start with very coarse groups (uscms01.users)] Real ACLs
challenging, can do something for "co-operative systems" like dCache.

CMS data protected from other experiments.

** Storage management and data serving

Assume storage service with internal catalogue, i.e. logical naming
structure mostly separate from actual physical storage layout (= no
actual disk mount points etc.).  A site, in particular a very small
site (laptop :-), may expose physical layout, but takes on full
management responsibility then.  Recommended to use highly reliable
and scalable storage services (dCache, Castor, xrootd, lcg stuff,
...).

Say something about SRM?  We expect everyone to operate SRM, including
transfers, space management, advisory delete, etc.  How much focus
needs to be put on reliable, deployable non-dCache SRM?

We don't do tape management, it's supposed to be hidden by SRM,
however allowing necessary efficient management / hinting.

Tactical vs. strategic storage.  Resource quotas.  Etc.

** Data management scopes

Local vs. global data management.  Means two separate things: a)
site-local data management vs. one done globally within CMS, and b)
data management done by individuals, groups and CMS as a whole, with
different ranges of data scope/visibility.

Division of work between CMS managing its own data, and system
operator management.  CMS manages the data logically; operators only
manage storage and file placement in pools etc. (SRM, SE and below).
Operators never need to manage CMS data logically; CMS manages space
etc. reservations with SRM-like tools.  Catalogues etc. are entirely
CMS domain.  CMS provides means to release file space when files are
no longer needed, e.g. after they have been made safe in Tier-1
custody.  CMS provides tools for (CMS) site data administrators.

Data management done by individuals, groups etc.  Including user
private data management -- same tools as everywhere else.  Production
also uses the same tools, just different scale.  Publishing data to
other individuals.  Using DBS functions/services locally in groups
working locally (e.g. in Tier-2), then later making the data available
publicly.  Propagating info up and down the chain: global, group,
local data management.  Basic line: same tools used everywhere, can be
hooked up together (e.g. can have local dataset bookkeeping system
which tracks own private datasets + mirrors information from cms-wide
info + mirrors local group's dataset bookkeeping as well, and all this
can later be published into the central database).  Scope of the data,
who sees data from any particular dataset bookkeeping system instance.

File custody handling.

User private file management (ntuples etc.).  Basically the same as
any other data we track and move about.

Moving vs. replicating files.  When files have made it to the
destination, remove original.  Can already be done with cleaner-type
agents.

** File and buffer management

Sorting events, luminosity.

How does data get out from online farm, buffer management.

MC data (pileup in one files, hit files, ... -> full event files).

File merging.

** Priorities and policies

Use private data management, local management.  Small MC jobs, ntuple
management.  Important use case, especially ntuple backup,
distribution to jobs for grids.  (Idea: treat output from job family
as a single data block, gravitate it all to a destination specified by
user.  Remove at original source (run cleaner) when made it to
destination.  Allow back-up transfer assignment to Tier-1 for backup.
When private data required as job input, create assignments of the
block(s) to destination nodes.  Job can be started when data is
reported as moved.  So this works exactly like the CMS-wide scale.)

"Outer rim" for production use.  There's an edge to the world where
CMS can run services.  Jobs can run outside that, and outside can be
represented as "virtual" nodes (e.g. USMOP), but files in catalogue
are arranged to be gsiftp://... from some "last edge of CMS", and are
downloaded to the worker node on the fly -- not placed/pushed to the
site before job, but fetched to tactical storage on the fly, then
removed after the job.

File naming conventions.  LFN space usage.

Data management in online environment?

Workflows etc.

Priorities as fallback solutions.

** Other stuff

 how to update ddd with alignment?


