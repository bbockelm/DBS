**********************************************************************
Data management documents
- Current system
- Baseline system
- Development guidance
- Organisation
   - Projects
   - Schedule
   - Operation

- Use cases / scenarios?


**********************************************************************
* Current CMS data management system

The functionality of the data management is currently achieved through
a number of components and services that are fairly loosely coupled.
While not exactly the data management system we expect to have on LHC
turn-on, it does include a number of design principles we intend to
maintain.

The core of the dataset bookkeeping system is formed by the virtual
data view and logical file catalogue parts of RefDB.  Instances of
PubDB extend this for site-local information.  [FIXME: Dataset
definitions: dataset, owner, collection?.  Various aspects of the
virtual data view and logical file catalogue in RefDB.  Exactly what
PubDB provides today.]

The data transfers are implemented using PhEDEx.  [FIXME: Expand a
little, layering from below.]

Each site has local POOL file catalogues; there is no global replica
catalogue in CMS.  Catalogue consistency is maintained between sites
as a part of the data transfer system.  In addition various
consistency checking tools are provided -- what PhEDEx thinks the site
has, what's on storage element (disk, tape), what's in local file
catalogue, what's been published in PubDB, what files RefDB knows
about.  The site-local catalogue for event data files is a RDBMS
(MySQL, Oracle) POOL file catalogue.  XML catalogues are used for
other files.

Storage systems: LCG SE, dCache, Castor, SRM/dCache, raw disk pools.

User interface: PHYSH, production web pages, command line tools.

**********************************************************************
* Baseline system

The data management project will evolve our current data management
system to a *baseline system* by end of 2006.  The baseline is an
implementation with demonstrated capability to support expected CMS
data processing needs in 2007 at LHC turn-on with available resources,
and demonstrated ability to evolve to meet new goals.

The schedule chapter describes how the required functionality and
evolution capabilities will be demonstrated over time.  The general
concept is to develop the baseline system by evolving and refactoring
existing services to better ones until we have the desired system,
maintaining a fully working service at all times.  Service challenges
are scheduled as milestones to demonstrate achieved levels of
functionality, scalability and convenience of operation and
maintenance.  Passing the milestones indicates both readiness to
handle CMS data processing needs and ability to evolve the system to
meet new goals.  [FIXME: backward/forward compatibility?]

The rationale for the baseline system is described in a separate
chapter.  In brief, we intend to retain the data management system
as a fairly loosely coupled set of services and components.

There is no fallback to the baseline solution as such.  Due to the
development model there will always be a working system that can be
used and developed to a different direction should it look like that
the data management project will be unable to deliver the desired
system in time.  Each service challenge milestone provides a project
checkpoint to determine if the development is on track and allows the
plans to be revisited.

The baseline system can be characterised as conservative; this is a
deliberate choice on our part.  New services and components may emerge
with desirable functionality richer than described in the baseline and
demonstrated sufficient robustness.  We anticipate to take advantage
of such components, while not risking the design of the baseline to
depend on them.

Scheduling, project organisation and other planning is described
in a separate chapter.

** System overview

FIXME: Deployment model -- how many instances, what's running where,
etc. (not only this service, but others as well.)

FIXME: Interactions with other systems / components / services.


** User access

PHYSH, web pages, command line tools.

** Framework interaction

Expand on what framework provides and receives.

** Dataset bookkeeping system

FIXME: Purpose for queries.

FIXME: Linkage with production system.

FIXME: Linkage with workload management system.

Tracks datasets at various level of detail: trigger stream / offline
stream / <various params > / chunk...  Chunk is the smallest atom of
data moved around (but for mc production files may originally be
scattered -- but then source will probably be forgotten). Chunks may
be open or closed; once closed, immutable.  Files, chunks and entire
datasets can carry various attribute tags (including "bad").  Consider
here different slices of data: depends on how many files we write out,
and which ones are needed for input of different types of jobs.

Tracks replicas of chunks.  Detail greater than this only known in
data transfer system and at sites involved.  Chunk replica tracking
based on peer-to-peer network where each site manages and controls
knowledge of what chunks exist at that site, plus communicates that to
its "neighbours" or central point.

Knows which files belong to which dataset -- logical file catalogue --
and attributes of those files such as size, checksum, luminosity, etc.
Doesn't know about file replicas however, this is handled by chunk
replicas and site-local catalogues.

Asynchronously learns about new files, i.e. there's some agents that
pick up job outputs at various sites and propagate that knowledge to
dataset bookkeeping system.  Or some other means of asynchronous
harvesting -- rgma etc.  Able to handle virtual data (create new
dataset, then start filling it in) as well as data adoption (produce
something then decide to make it known; may be refused if parametres
weren't correct).  Data transfer operates asynchronously of these
processes.

FIXME: File merging.

** Parametre system

** Conditions data

** Data transfer system

Much like PhEDEx is now.  Will move to peer-to-peer, with T0/T1s
possibly sharing a database.  Needs to be trivial to install and
operate/maintain, 0.0N FTE.

Push/pull/stream modes.  Pure data transfer system, not a replica
catalogue, not rest of "workflow".  Scales by # of files in transfer.
Leverages data chunking at higher level -- but doesn't require a chunk
to exist anywhere in entirety.  Robust, internet-like design, no one
part can bring entire transfer network down.  Maintains abstractions
at different layers.  Should be able to saturate any hardware
(network, storage) thrown at it.  Operates completely asynchronously
from other data management systems, production, worker nodes, ...

Layers/modules:
1. Unreliable point-to-point file transfer: globus-url-copy, srmcp, ...
2. Reliable point-to-point (= single-hop) transfer, local resource management
3. Reliable routed (= multi-hop) transfer
4. Dataset/chunk-level data transfer
4.a. Allocate files to destinations, back-up routing etc.
4.b. Monitor transfers at chunk level, notify site on progress
4.c .Activate/deactivate chunks
5. Placing files into transfer
5.a. Production link: harvest files from completed jobs
5.b. Bulk transfer requests: pick transfer assignments

** Validation

** File catalogues

Two separate aspects here: catalogue client and implementation.
Client is POOL interface.  Implementation may be local look-up table
(no service) or site-local file catalogue based on relational
database.  Auxiliary catalogues (XML) may be used, either accessed
directly from storage or downloaded to worker node with http/gsiftp
(external connectivity not required).  The catalogue used to record
and look up replica paths, nothing else.

The catalogue is to be entirely *local*, but may contain several
replicas for local load balancing (is this in conflict with previous
point of logical name spaces?).  Catalogues are updated and accessed
by the data transfer system (update a no-op for lookup table).  We do
not use catalogues for cross-site replica discovery.

** Storage management

Assume storage service with internal catalogue, i.e. logical naming
structure mostly separate from actual physical storage layout (= no
actual disk mount points etc.).  A site, in particular a very small
site (laptop :-), may expose physical layout, but takes on full
management responsibility then.  Recommended to use highly reliable
and scalable storage services (dCache, Castor, xrootd, lcg stuff,
...).

Say something about SRM?

File merging (actually a lot more than just storage!)

Tape.

** ACLs

**********************************************************************
* Development guidance and process

The baseline data management system design incorporates requirements
from use cases (see separate chapter) and our experience of what will
reasonably work at large scale.  That is, we apply our experience to
deliver a data management system which gives CMS a competitive edge by
providing reliable, adaptable and scalable access to data.  This
chapter outlines the strategic choices we believe to be essential for
this.

New components proposed for inclusion into the data management system
must have proven record of reliability and scalability in real-life
service challenges.  In other words technology must be deemed both
desirable and validated to be considered for inclusion.  We believe
we have all the technical knowledge to implement the baseline system:
either the components already exist, or we have a clear idea of how
to design and develop them.  Hence we do not expect research to play
a significant role in delivering the baseline data management system.
The most important work of the project is to deliver a verified
operational environment.

The baseline is system will be developed by evolving and refactoring
existing services.  This will ensure for one that there is always a
working system, and for other teaches us how to develop a deployed
system including planning and execution of version upgrades.

[FIXME: backward/forward compatibility?]

[FIXME: loosely coupled set of services and components.]

** Strategical choices

We conclude the following from our experience:

1) We have demonstrated ability to run high-availability local
   services in few areas: disk servers, mail system, databases,
   limited number of a web services.  Few centres are able to provide
   such services with high reliability: this is often proportional to
   manpower available to ensure smooth operation.

2) We have not demonstrated ability to deliver high-availability
   non-local services except in very few areas: internet
   infrastructure services (mail, dns), web to some extent.

3) It takes at least a year to go from concept to delivering reliable
   service when *underlying design was sound to begin with.* It takes
   considerably longer if one has to iterate to get to sound design.

We believe the baseline data management system must be drafted
conservatively to account for these factors, while leaving open the
option to exploit future technologies as they mature.

When considering how services ought to be distributed, we select as
design guidance a) required degree of synchronisation, b) sensitivity
to failures, and c) intensity of operations (frequency, bandwidth).
No service should be designed to rank highly in all three aspects.
The lower the ranks, the more reliable and scalable the system: the
better it is capable of delivering physics.  The higher the rank in
any area, the closer the service ought to be to the client.

For example:

1) Selecting data to run on requires low level of synchronisation (=~
   correct within hours or days).  It's sensitive to failures in that
   the job won't get done -- but the human can always come back later.
   Intensity is low.  This could be handled by central service,
   distribution would add robustness in adding fallbacks.

2) Any service accessed by worker node job is highly sensitive in all
   three areas.  Therefore we should minimise such services, or design
   them in ways to lower the ranks.  For example where database
   service reliability is bound to be low, caching-based solutions are
   better (asynchronous caching is even better).  Data I/O and file
   name lookup require extremely reliable services.

3) Collecting job results can be made less risky by doing it
   separately from the job itself where that is an option, failing
   that by making fewest possible connections to most reliable
   possible services from the job.

Where services need to be brought close to the client, we need guides
for sound choices.  Four main approaches we consider to be scalable
and reliable:

1) Include only what is truly needed.  Eliminate services that are not
   required, or arrange them to be required only in special cases, not
   in the common uses of the system.

2) Partition the problem.  [FIXME: Address authorative information
   sources.]

2.a) If data is local, keep it local.  If it's purely local, don't
  expose it outside.  If access is required from outside, it may
  simply be a whole lot more expensive to get at it.

2.b) If the data can be layered or sliced, do so.  This allows
  different segments to be addressed differently, and even to be
  ignored.  For instance higher-level data may be replicated
  everywhere asynchronously, and only details about "interesting bits"
  stored locally.  Combination with 2.a makes things even better.

3) Cache; update via separate channel if necessary.  Frontier and
   using web cache-type technology.

4) Peer-to-peer distribution.  May or may not have a central master.
   These are similar to core internet services.  Cost to access
   information or available detail may vary with distance to the
   source.

**********************************************************************
* Organisation

** Projects

** Schedule
*** Intermediate goals (now, six months, baseline)

** Operation

**********************************************************************
* Use cases and scenarios

Queries
Workflows
