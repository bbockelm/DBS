**********************************************************************
* Development strategy

The baseline data management system design incorporates requirements
from use cases (see separate chapter) and our experience of what will
reasonably work at large scale.  That is, we apply our experience to
deliver a data management system which gives CMS a competitive edge by
providing reliable, adaptable and scalable access to data.  This
chapter outlines the strategic choices we believe to be essential for
this.

New components proposed for inclusion into the data management system
must have proven record of reliability and scalability in real-life
integration tests.  In other words technology must be deemed both
desirable and validated to be considered for inclusion.  We believe we
have all the technical knowledge to implement the baseline system:
either the components already exist, or we have a clear idea of how to
design and develop them.  Hence we do not expect research to play a
significant role in delivering the baseline data management system.
The most important work of the project is to deliver a verified
operational environment.

The baseline is system will be developed by evolving and refactoring
existing services.  This will ensure for one that there is always a
working system, and for other teaches us how to develop a deployed
system including planning and execution of version upgrades.

[FIXME: backward/forward compatibility?]

[FIXME: loosely coupled set of services and components.]

** Strategical choices

We conclude the following from our experience:

1) We have demonstrated ability to run high-availability local
   services in few areas: disk servers, mail system, databases,
   limited number of a web services.  Few centres are able to provide
   such services with high reliability: this is often proportional to
   manpower available to ensure smooth operation.

2) We have demonstrated ability to deliver high-availability non-local
   services only in very few areas: internet infrastructure services
   (mail, dns), web to some extent.

3) It takes at least a year to go from concept to delivering reliable
   service when *underlying design was sound to begin with.* It takes
   considerably longer if one has to iterate to get to sound design.

4) [FIXME: Need to migrate systems in a coordinated manner -- taking
    people and systems ahead, but not breaking previous generation
    unnecessarily.  Issues with migrating experiment as a whole
    through both software and system generations: it's not that
    different whether it is reconstruction software generation,
    or migrating our system services.  Tension among inability to
    run distributed services (logic in service), client proliferation
    (logic in clients), service upgrades (logic in intermediate
    service libraries/toolkits/...).  State clear conclusions that
    can be used to derive guidance.]

We have accounted for these by describing a baseline system that is
conservative regarding these factors, but partitioned such that we
retain the option to exploit more advanced services in future as they
mature.

When considering how services ought to be distributed, we select as
design guidance a) required degree of synchronisation, b) sensitivity
to failures, and c) intensity of operations (frequency, bandwidth),
and d) the degree to which two ends of the communication must agree on
protocol.  No service should be designed to rank highly in all four
aspects.  The lower the ranks, the more reliable, scalable and
maintainable the system: the better it is capable of delivering
physics.  The higher the rank in any area, the closer the service
ought to be to the client, or the harder it is to upgrade systems.

For example:

1) Selecting data to run on requires low level of synchronisation (=~
   correct within hours or days).  It's sensitive to failures in that
   the job won't get done -- but the human can always come back later.
   Intensity is low.  This could be handled by central service,
   distribution would add robustness in adding fallbacks.

2) Any service accessed by worker node job is highly sensitive in all
   three areas.  Therefore we should minimise such services, or design
   them in ways to lower the ranks.  For example where database
   service reliability is bound to be low, caching-based solutions are
   better (asynchronous caching is even better).  Data I/O and file
   name lookup require services that at least as reliable as the
   underlying storage service -- a good reason to unify the two.

3) Collecting job results can be made less risky by doing it
   separately from the job itself where that is an option, failing
   that by making fewest possible connections to most reliable
   possible services from the job.

Where services need to be brought close to the client, we need guides
for sound choices.  The main approaches we consider to be scalable
and reliable:

1) Keep operated systems simple.  Eliminate services that are not
   required, and share technology for different services where it
   can be done sensibly.  Actual exploited systems should not vary
   unnecessarily.

2) Include only what is truly needed.  The existence of every
   component and service must be justifiable in the context of the
   entire end-to-end system, accounting both advantages as well as
   risks introduced by the component.

3) Partition the problem.  [FIXME: Address authorative information
   sources.]

3.a) If data is local, keep it local.  If it's purely local, don't
  expose it outside.  If access is required from outside, it may
  simply be a whole lot more expensive to get at it.

3.b) If the data can be layered or sliced, do so.  This allows
  different segments to be addressed differently, and even to be
  ignored.  For instance higher-level data may be replicated
  everywhere asynchronously, and only details about "interesting bits"
  stored locally.  Combination with 2.a makes things even better.

4) Cache; update via separate channel if necessary.  Frontier and
   using web cache-type technology.

5) Peer-to-peer distribution.  May or may not have a central master.
   These are similar to core internet services.  Cost to access
   information or available detail may vary with distance to the
   source.

6) Encapsulating "business logic" either in servers, or bundled
   libraries; thin clients; protocol conformance.  Versus auto-update
   technology.

[FIXME: Simplify where current model / workflows are complex.  Don't
build a model that capitalises on manpower and diligence of people.
Avoid error-prone procedures.]

**********************************************************************
* Baseline system

The baseline system documented in this chapter is based on our
current understanding of how to provide a system with demonstrated
capability to support the expected CMS data processing needs for
the two first years following LHC turn-on in 2007, and demonstrated
ability to evolve to meet new goals.

The schedule chapter describes how the required functionality and
evolution properties will be demonstrated.  In short, the baseline
system will be evolved and refactored from the present existing
services to the desired system, continually maintaining a fully
operational system.  Development is focused on integration testbed
milestones approximately every three months.  A prescribed level of
functionality, scalability and convenience of operation and
maintenance must be demonstrated at these testbeds.  In between the
milestones smaller improvements to the operational systems will be
made.  Passing the milestone indicates both readiness to handle a new
level of CMS data processing needs, and ability to evolve the systems
to meet new goals. [FIXME: project checkpoint] [FIXME:
backward/forward compatibility.]

[FIXME: integration testbeds, challenges, certification, see
scheduling chapter.]

The rationale for the baseline system is described in a separate
chapter.  In brief, we intend to retain the data management system
as a fairly loosely coupled set of services and components.

There is no fallback to the baseline solution as such.  Due to the
development model there will always be a working system that can be
used and developed to a different direction should it look like that
the data management project will be unable to deliver the desired
system in time.  Each service challenge milestone provides a project
checkpoint to determine if the development is on track and allows the
plans to be revisited.

The baseline system can be characterised as conservative; this is a
deliberate choice on our part.  New services and components may emerge
with desirable functionality richer than described in the baseline and
demonstrated sufficient robustness.  We anticipate to take advantage
of such components, while not risking the design of the baseline to
depend on them.

Scheduling, project organisation and other planning is described
in a separate chapter.

** System overview

FIXME: System overview, deployment model, interactions.  How many
instances of what are running where, which systems, services or
components they interact with and how.

** User access

Data discovery, queries, web pages, command line tools.  PHYSH like
tools.

** Framework interaction

Expand on what framework provides and receives.

*** Dataset bookkeeping system

*** Data Organization

FIXME: Brief description on the fundamental assumptions on data
organization: event model, vertical streams and production datasets,
mapping of events to files, etc

*** Datasets

FIXME: Dataset definition, relationships, virtual data, dataset
creation, etc.

Datasets consist of a collection of events of a certain type (as
detemined by physics meta-data, like their trigger path or physics
generator). Datasets become streams of eventdata for processing
(input/output) by specifying an event range (by giving a *constraint*)
and a event representation/processing step (?) (by specifying a
*processing ID*), see below.

Physics users thus query the physics metadata to pick and discover
datasets of their choosing, and they investigate physics metadata
available for a data sets.

Physics Metadata relates to a dataset or a subset of events from a
dataset, and is distinct from other metadata, like the e.g. file-level
metadata etc.

The collection of events that belong to a dataset is not fixed, but
will change during data taking or when more events are being added for
simulated data sets.

To define a specific representation of a specific collection of CMS
events, like when specifying an RAW or AOD input stream for event
processing, one would need specify

-- a dataset identifier (DSID) 

-- a set of constraints (even range, run range, validity period, data
   quality requirements, integrated luminosity etc)

-- a parameterset identifier, which defines the processing that the
   event data in the event [FIXME needs work on how these entities get
   defined, see EDM]

Each dataset has a unique DSID. A physics user obtains a DSID by going
to the datset bookkeeping system DBS and querying the physics
metadata, or by resolving a production assignment ID etc.

FIXME: Purpose for queries

Describe role and structure of physics metadata as attributes of data
sets, that gets queried

  Not file level (Block stuff), cvs-like tags
  Open / closed
  Attribute tags
  Logical file catalogue: size, checksum, conditions links?
  Replica tracking
   at blocks level, site ownership
   sites specify what data they serve?

  Queries
  Linkage with production: preparation, summary, file registration,
   dataset creation, importing datasets wholesale; asynchronous
  Linkage with workload management
  Linkage with transfer -- blocks as replication unit
  File merging



FIXME: Linkage with production system.

providing dataset "transactions" like "create new dataset" and
bookkeeping of dataset-related metadata.

coupling to production processing (and eventually data taking) through
"importing" of files into a dataset

FIXME: Linkage with workload management system.

FIXME: chunks are important in practice, but should probably be
treated as an optimization vehicle for data placement?

FIXME: Linkage with other systems -- conditions, lumi information, etc.


FIXME: Not just file-level

Tracks datasets at various level of detail: trigger stream / offline
stream / <various params > / chunk...  Chunk is the smallest atom of
data moved around (but for mc production files may originally be
scattered -- but then source will probably be forgotten). Chunks may
be open or closed; once closed, immutable.  Files, chunks and entire
datasets can carry various attribute tags (including "bad").  Consider
here different slices of data: depends on how many files we write out,
and which ones are needed for input of different types of jobs.

Tracks replicas of chunks.  Detail greater than this only known in
data transfer system and at sites involved.  Chunk replica tracking
based on peer-to-peer network where each site manages and controls
knowledge of what chunks exist at that site, plus communicates that to
its "neighbours" or central point.

Knows which files belong to which dataset -- logical file catalogue --
and attributes of those files such as size, checksum, luminosity, etc.
Doesn't know about file replicas however, this is handled by chunk
replicas and site-local catalogues.

Asynchronously learns about new files, i.e. there's some agents that
pick up job outputs at various sites and propagate that knowledge to
dataset bookkeeping system.  Or some other means of asynchronous
harvesting -- rgma etc.  Able to handle virtual data (create new
dataset, then start filling it in) as well as data adoption (produce
something then decide to make it known; may be refused if parametres
weren't correct).  Data transfer operates asynchronously of these
processes.

FIXME: I don't particularly like the formulation "learns about" and
the agents pardigm for this one. I thought a production job could
"check in" or import a file to a dataset, which is a transaction of
the system, and from then on does not have to deal any more with the
specific file? If there is no such transaction how can the production
job finish and release storage etc?

FIXME: File merging. Q: What happens with chunks when files get merged?

** Parametre system

Detail: My understanding is that event format like RAW or AOD is given
by the parameter ID?

FIXME: relationship to current owner information
FIXME: Also stores all parameters used by each algorithm?

** Conditions data

Interaction with...

** Data transfer system

Much like PhEDEx is now.  Will move to peer-to-peer, with T0/T1s
possibly sharing a database.  Needs to be trivial to install and
operate/maintain, 0.0N FTE.

Push/pull/stream modes.  Pure data transfer system, not a replica
catalogue, not rest of "workflow".  Scales by # of files in transfer.
Leverages data chunking at higher level -- but doesn't require a chunk
to exist anywhere in entirety.  Robust, internet-like design, no one
part can bring entire transfer network down.  Maintains abstractions
at different layers.  Should be able to saturate any hardware
(network, storage) thrown at it.  Operates completely asynchronously
from other data management systems, production, worker nodes, ...

Layers/modules:
1. Unreliable point-to-point file transfer: globus-url-copy, srmcp, ...
2. Reliable point-to-point (= single-hop) transfer, local resource management
3. Reliable routed (= multi-hop) transfer
4. Dataset/chunk-level data transfer
4.a. Allocate files to destinations, back-up routing etc.
4.b. Monitor transfers at chunk level, notify site on progress
4.c .Activate/deactivate chunks
5. Placing files into transfer
5.a. Production link: harvest files from completed jobs
5.b. Bulk transfer requests: pick transfer assignments

** Validation

Technical validation.
Transfer validation.
Physics validation, including calibration validations.
[Tony/Nicola have much more detailed division.]

** File catalogues and access authorization

Two separate aspects here: catalogue client and implementation.
Client is POOL interface.  Implementation may be local look-up table
(no service) or site-local file catalogue based on relational
database.  Auxiliary catalogues (XML) may be used, either accessed
directly from storage or downloaded to worker node with http/gsiftp
(external connectivity not required).  The catalogue used to record
and look up replica paths, nothing else.

The catalogue is to be entirely *local*, but may contain several
replicas for local load balancing (is this in conflict with previous
point of logical name spaces?).  Catalogues are updated and accessed
by the data transfer system (update a no-op for lookup table).  We do
not use catalogues for cross-site replica discovery.

FIXME: in major practical cases like the Castor or dCache based
storage elements that functionality is basically provided by the SE
"names space" (PNFS in case of dCache). These systems are already
highly optimized and robust (or at least equally robust as the storage
itself), so we probably should directly use them. One way of doing
that is to use URL-style file names everywhere, which also (as far as
I understand) provides access control etc.

FIXME: access control lists, authorization.  Role-based security?
Can ACLs be implemented robustly at all at timescale of baseline?
[FIXME: Ian Fisk's comments on role-based mapping to GIDs in UID.GID
standard POSIX security model.]

** Storage management and data serving

Assume storage service with internal catalogue, i.e. logical naming
structure mostly separate from actual physical storage layout (= no
actual disk mount points etc.).  A site, in particular a very small
site (laptop :-), may expose physical layout, but takes on full
management responsibility then.  Recommended to use highly reliable
and scalable storage services (dCache, Castor, xrootd, lcg stuff,
...).

Say something about SRM?  We expect everyone to operate SRM, including
transfers, space management, advisory delete, etc.

We don't do tape management, it's supposed to be hidden by SRM,
however allowing necessary efficient management / hinting.

Tactical vs. strategic storage.  Resource quotas.  Etc.

** Data management scopes

Local vs. global data management.  Means two separate things: a)
site-local data management vs. one done globally within CMS, and b)
data management done by individuals, groups and CMS as a whole, with
different ranges of data scope/visibility.

Division of work between CMS managing its own data, and system
operator management.  CMS manages the data logically; operators only
manage storage and file placement in pools etc. (SRM, SE and below).
Operators never need to manage CMS data logically; CMS manages space
etc. reservations with SRM-like tools.  Catalogues etc. are entirely
CMS domain.  CMS provides means to release file space when files are
no longer needed, e.g. after they have been made safe in Tier-1
custody.  CMS provides tools for (CMS) site data administrators.

Data management done by individuals, groups etc.  Including user
private data management -- same tools as everywhere else.  Production
also uses the same tools, just different scale.  Publishing data to
other individuals.  Using DBS functions/services locally in groups
working locally (e.g. in Tier-2), then later making the data available
publicly.  Propagating info up and down the chain: global, group,
local data management.  Basic line: same tools used everywhere, can be
hooked up together (e.g. can have local dataset bookkeeping system
which tracks own private datasets + mirrors information from cms-wide
info + mirrors local group's dataset bookkeeping as well, and all this
can later be published into the central database).  Scope of the data,
who sees data from any particular dataset bookkeeping system instance.

File custody handling.

User private file management (ntuples etc.).  Basically the same as
any other data we track and move about.


** Priorities and policies

Workflows etc.

Priorities as fallback solutions.

** Job monitoring (under workflow management)

**********************************************************************
* Current CMS data management system

The functionality of the data management is currently achieved through
a number of components and services that are fairly loosely coupled.
While not exactly the data management system we expect to have on LHC
turn-on, it does include a number of design principles we intend to
maintain.

The core of the dataset bookkeeping system is formed by the virtual
data view and logical file catalogue parts of RefDB.  Instances of
PubDB extend this for site-local information.  [FIXME: Dataset
definitions: dataset, owner, collection?.  Various aspects of the
virtual data view and logical file catalogue in RefDB.  Exactly what
PubDB provides today.]

The data transfers are implemented using PhEDEx.  [FIXME: Expand a
little, layering from below.]

Each site has local POOL file catalogues that has several
functions. The set of files that belong to the datasets stored at a
given sites are implcitly defined through the POOL pointer structure
-- all files in a dataset will "by magic" be contained in the dataset
POOL catalog, which also specifies the physical location of files
pointed to by event references in the collection.

There is no global replica 
catalogue in CMS.  Catalogue consistency is maintained between sites
as a part of the data transfer system.  In addition various
consistency checking tools are provided -- what PhEDEx thinks the site
has, what's on storage element (disk, tape), what's in local file
catalogue, what's been published in PubDB, what files RefDB knows
about.  The site-local catalogue for event data files is a RDBMS
(MySQL, Oracle) POOL file catalogue.  XML catalogues are used for
other files.

Storage systems: LCG SE, dCache, Castor, SRM/dCache, raw disk pools.

User interface: PHYSH, production web pages, command line tools.

**********************************************************************
* Organisation

** Projects

Champions + members.  Broken down to fairly small projects, any one
person might champion two projects and be a member in three or four
others.  Champion responsible for reporting, calling meetings, etc.
Otherwise everybody works in peer structure.  Some projects might be
short-lived, others might be longer-lived.

** Schedule

FIXME: Expanded version of the bits about scheduling in baseline
description.

FIXME: I think we should describe the processes of development of new
components that get then integrated and eventually moved into
production. There is an integration testbed and a set of services
challenges that prove a new revision of services and functionalities,
performed in an integration environment, that then can be released for
production when certified. etc pp

*** Intermediate goals (now, six months, baseline)

Now.  Summer.  Magnet test ~ Oct/Nov.  DC06.  End of 2006.  April 2007.

Baseline in place for DC06.

Integration testbed plans.

** Operation

Migration policy.  New EDM.

**********************************************************************
* Use cases, scenarios, requirements, workflows

scenarios for data taking, simulation production, processing
production, physics analysis

scenarios for production-level creation and manipulation of datasets
ditto for physics user-level cration and manipulation of datasets

scenarios of physics user-driven movement of datasets to T2s, laptops
-- including deployment scenarios

Import
Queries

Workflows
(validation) data management system independent of data quality
 - technical validation vs. physics validation
 - dms tracks separately bad (corrupted/lost/...) data and
    physics attributes
 - do we need workflow hooks for initiating physics validation?
