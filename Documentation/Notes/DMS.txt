**********************************************************************
* Development strategy

The baseline data management system design incorporates requirements
from use cases (see separate chapter) and our experience of what will
reasonably work at large scale.  That is, we apply our experience to
deliver a data management system which gives CMS a competitive edge by
providing reliable, adaptable and scalable access to data.  This
chapter outlines the strategic choices we believe to be essential for
this.

New components proposed for inclusion into the data management system
must have proven record of reliability and scalability in real-life
integration tests.  In other words technology must be deemed both
desirable and validated to be considered for inclusion.  We believe we
have all the technical knowledge to implement the baseline system:
either the components already exist, or we have a clear idea of how to
design and develop them.  Hence we do not expect research to play a
significant role in delivering the baseline data management system.
The most important work of the project is to deliver a verified
operational environment.

The baseline system will be developed by evolving and refactoring
existing services.  This will ensure for one that there is always a
working system, and for other teaches us how to develop a deployed
system including planning and execution of version upgrades.

[FIXME: backward/forward compatibility?]

[FIXME: loosely coupled set of services and components.]

** Strategical choices

We conclude the following from our experience:

1) We have demonstrated ability to run high-availability local
   services in few areas: disk servers, mail system, databases,
   limited number of a web services.  Few centres are able to provide
   such services with high reliability: this is often proportional to
   manpower available to ensure smooth operation.

2) We have demonstrated ability to deliver high-availability non-local
   services only in very few areas: internet infrastructure services
   (mail, dns), web to some extent.

3) It takes at least a year to go from concept to delivering reliable
   service when *underlying design was sound to begin with.* It takes
   considerably longer if one has to iterate to get to sound design.

4) [FIXME: Need to migrate systems in a coordinated manner -- taking
    people and systems ahead, but not breaking previous generation
    unnecessarily.  Issues with migrating experiment as a whole
    through both software and system generations: it's not that
    different whether it is reconstruction software generation,
    or migrating our system services.  Tension among inability to
    run distributed services (logic in service), client proliferation
    (logic in clients), service upgrades (logic in intermediate
    service libraries/toolkits/...).  State clear conclusions that
    can be used to derive guidance.]

We have accounted for these by describing a baseline system that is
conservative regarding these factors, but partitioned such that we
retain the option to exploit more advanced services in future as they
mature.

When considering how services ought to be distributed, we select as
design guidance a) required degree of synchronisation, b) sensitivity
to failures, and c) intensity of operations (frequency, bandwidth),
and d) the degree to which two ends of the communication must agree on
protocol.  No service should be designed to rank highly in all four
aspects.  The lower the ranks, the more reliable, scalable and
maintainable the system: the better it is capable of delivering
physics.  The higher the rank in any area, the closer the service
ought to be to the client, or the harder it is to upgrade systems.

For example:

1) Selecting data to run on requires low level of synchronisation (=~
   correct within hours or days).  It's sensitive to failures in that
   the job won't get done -- but the human can always come back later.
   Intensity is low.  This could be handled by central service,
   distribution would add robustness in adding fallbacks.

2) Any service accessed by worker node job is highly sensitive in all
   three areas.  Therefore we should minimise such services, or design
   them in ways to lower the ranks.  For example where database
   service reliability is bound to be low, caching-based solutions are
   better (asynchronous caching is even better).  Data I/O and file
   name lookup require services that at least as reliable as the
   underlying storage service -- a good reason to unify the two.

3) Collecting job results can be made less risky by doing it
   separately from the job itself where that is an option, failing
   that by making fewest possible connections to most reliable
   possible services from the job.

Where services need to be brought close to the client, we need guides
for sound choices.  The main approaches we consider to be scalable
and reliable:

1) Keep operated systems simple.  Eliminate services that are not
   required, and share technology for different services where it
   can be done sensibly.  Actual exploited systems should not vary
   unnecessarily.

2) Include only what is truly needed.  The existence of every
   component and service must be justifiable in the context of the
   entire end-to-end system, accounting both advantages as well as
   risks introduced by the component.

3) Partition the problem.  [FIXME: Address authorative information
   sources.]

3.a) If data is local, keep it local.  If it's purely local, don't
  expose it outside.  If access is required from outside, it may
  simply be a whole lot more expensive to get at it.

3.b) If the data can be layered or sliced, do so.  This allows
  different segments to be addressed differently, and even to be
  ignored.  For instance higher-level data may be replicated
  everywhere asynchronously, and only details about "interesting bits"
  stored locally.  Combination with 2.a makes things even better.

4) Cache; update via separate channel if necessary.  Frontier and
   using web cache-type technology.

5) Peer-to-peer distribution.  May or may not have a central master.
   These are similar to core internet services.  Cost to access
   information or available detail may vary with distance to the
   source.

6) Encapsulating "business logic" either in servers, or bundled
   libraries; thin clients; protocol conformance.  Versus auto-update
   technology.

[FIXME: Simplify where current model / workflows are complex.  Don't
build a model that capitalises on manpower and diligence of people.
Avoid error-prone procedures.]

**********************************************************************
* Baseline system

The baseline system documented in this chapter is based on our
current understanding of how to provide a system with demonstrated
capability to support the expected CMS data processing needs for
the two first years following LHC turn-on in 2007, and demonstrated
ability to evolve to meet new goals.

The schedule chapter describes how the required functionality and
evolution properties will be demonstrated.  In short, the baseline
system will be evolved and refactored from the present
services to the desired system, continually maintaining a fully
operational system.  Development is focused on integration testbed
milestones approximately every three months.  A prescribed level of
functionality, scalability and convenience of operation and
maintenance must be demonstrated at these testbeds.  In between the
milestones smaller improvements to the operational systems will be
made.  Passing the milestone indicates both readiness to handle a new
level of CMS data processing needs, and ability to evolve the systems
to meet new goals. [FIXME: project checkpoint] [FIXME:
backward/forward compatibility.]

[FIXME: integration testbeds, challenges, certification, see
scheduling chapter.]

The rationale for the baseline system is described in the previous
chapter.  In brief, we intend to retain the data management system as
a fairly loosely coupled set of services and components where critical
communication is in general asynchronous and agent-based.

There is no fallback to the baseline solution as such.  Due to the
development model there will always be a working system that can be
used and developed to a different direction should it look like that
the data management project will be unable to deliver the desired
system in time.  Each service challenge milestone provides a project
checkpoint to determine if the development is on track and allows the
plans to be revisited.

The baseline system can be characterised as conservative; this is a
deliberate choice on our part.  New services and components may emerge
with desirable functionality richer than described in the baseline and
demonstrated sufficient robustness.  We anticipate to take advantage
of such components, while not risking the design of the baseline to
depend on them.

Scheduling, project organisation and other planning is described
in a separate chapter.

FIXME: need to specify what is free for site to choose

FIXME: CMS manages its own data in some large chunks.  CMS provides
the system that tracks where the data is, and makes decisions on
placements etc.  How to access the data is found out when jobs arrive
at the site.

** System overview

FIXME: System overview, deployment model, interactions.  How many
instances of what are running where, which systems, services or
components they interact with and how.

** User access

Data discovery, queries, web pages, command line tools.  PHYSH like
tools.

** Framework interaction

Expand on what framework provides and receives.

Environment for which we used to build tools no longer exists (private
farms, basic bookkeeping etc.).  Will exist even less after new EDM.
Prune the old stuff out from both framework, production tools, overall
in the system.  Work out how parameters from data management /
production assignments etc. get provided to the jobs, get information
back out the other way.  In general moving much of this under DM
tools, less so in WM/production tools.

Running entire chains of jobs (cmkin, oscar, orca, dst, ntuple).


*** Dataset bookkeeping system

*** Data Organization

FIXME: Brief description on the fundamental assumptions on data
organization: event model, vertical streams and production datasets,
mapping of events to files, etc

*** Datasets

FIXME: Dataset definition, relationships, virtual data, dataset
creation, etc.

Datasets consist of a collection of events of a certain type (as
detemined by physics meta-data, like their trigger path or physics
generator). Datasets become streams of eventdata for processing
(input/output) by specifying an event range (by giving a *constraint*)
and a event representation/processing step (?) (by specifying a
*processing ID*), see below.

Physics users thus query the physics metadata to pick and discover
datasets of their choosing, and they investigate physics metadata
available for a data sets.

Physics Metadata relates to a dataset or a subset of events from a
dataset, and is distinct from other metadata, like the e.g. file-level
metadata etc.

The collection of events that belong to a dataset is not fixed, but
will change during data taking or when more events are being added for
simulated data sets.

To define a specific representation of a specific collection of CMS
events, like when specifying an RAW or AOD input stream for event
processing, one would need specify

-- a dataset identifier (DSID) 

-- a set of constraints (even range, run range, validity period, data
   quality requirements, integrated luminosity etc)

-- a parameterset identifier, which defines the processing that the
   event data in the event [FIXME needs work on how these entities get
   defined, see EDM]

Each dataset has a unique DSID. A physics user obtains a DSID by going
to the datset bookkeeping system DBS and querying the physics
metadata, or by resolving a production assignment ID etc.

FIXME: Purpose for queries

Describe role and structure of physics metadata as attributes of data
sets, that gets queried

FIXME: Run definition.

*** Bookkeeping system

Data atoms, unbreakable self-consistent unit of data (but consider
data hierarchy and dependencies), unique key.  Could be different for
data management and workload management (blocks vs. sets of runs) =>
two-way negotiation for best job split.  Compromise between user
desire of well-defined chunk and scalability.  Main issue: granularity
of data management vs. workload management vs. application itself.
User specification of input data dm and wm must refine into something
that is then passed to application -- what is the correct "chunk" --
LHC fill?  it becomes the set application needs to run on.

For WM data atoms are abstract and somehow lead to files to be
provided to the application to run on; wm doesn't know what files are
in an atom, dm does; atom itself is the smallest unit delivered to an
application.  Today: run / tier  + meta data => maps to N files.

virtual data catalogue
logical file catalogue
various other catalogues (runss, files, blocks)

not purely file-level -- different level of details costs different
amount.  some things are done at higher (3 or more orders of magnitude
more) level units like blocks; getting info below that level is not
available in dbs.

cvs-like tags.

open-closed datasets / blocks / whatever units

attribute tags -- data quality etc.  may apply to entire datasets as
well as smaller units, e.g. runs.

Logical file catalogue: size, checksum, conditions links?

Replica tracking philosophy -- at blocks level, site ownership of this
data, how it propagates back to central database (through agents?).
sites specify what data they serve, or specify what they have?

Queries

*** Linkage with other systems

Production: preparation, summary, file registration, dataset creation,
import datasets wholesale; providing dataset "transactions" like
"create new dataset" and bookkeeping of dataset-related metadata;
coupling to production processing (and eventually data taking) through
"importing" of files into a dataset; asynchronous update of
information.

Linkage with workload management; chunks are important in practice,
but should probably be treated as an optimization vehicle for data
placement?

Linkage with transfer -- blocks as replication unit

Linkage with other systems -- conditions, lumi information, etc.

Linkage with data quality tracking.

*** Operational behaviour

Asynchronous implementation through daemon/agent like things,
instead of active push/pull of information.


-------

FIXME: Not just file-level

Tracks datasets at various level of detail: trigger stream / offline
stream / <various params > / chunk...  Chunk is the smallest atom of
data moved around (but for mc production files may originally be
scattered -- but then source will probably be forgotten). Chunks may
be open or closed; once closed, immutable.  Files, chunks and entire
datasets can carry various attribute tags (including "bad").  Consider
here different slices of data: depends on how many files we write out,
and which ones are needed for input of different types of jobs.

Tracks replicas of chunks.  Detail greater than this only known in
data transfer system and at sites involved.  Chunk replica tracking
based on peer-to-peer network where each site manages and controls
knowledge of what chunks exist at that site, plus communicates that to
its "neighbours" or central point.

Knows which files belong to which dataset -- logical file catalogue --
and attributes of those files such as size, checksum, luminosity, etc.
Doesn't know about file replicas however, this is handled by chunk
replicas and site-local catalogues.

Asynchronously learns about new files, i.e. there's some agents that
pick up job outputs at various sites and propagate that knowledge to
dataset bookkeeping system.  Or some other means of asynchronous
harvesting -- rgma etc.  Able to handle virtual data (create new
dataset, then start filling it in) as well as data adoption (produce
something then decide to make it known; may be refused if parametres
weren't correct).  Data transfer operates asynchronously of these
processes.

FIXME: I don't particularly like the formulation "learns about" and
the agents pardigm for this one. I thought a production job could
"check in" or import a file to a dataset, which is a transaction of
the system, and from then on does not have to deal any more with the
specific file? If there is no such transaction how can the production
job finish and release storage etc?
----


FIXME: File merging. File size a function of limits on a single job.
Does merging retain file parentage information; when does it occur,
what are the constraints?  Q: What happens with chunks when files get
merged?  Impact on cross-file references (not allowed to merge after
references to files outside certain set have been made?)  Implications
on lost files -- merged related files should be constrained to be
synchronised in contents so these are easier to track?

FIXME: Data management system should account for what constitutes a
dataset.  The concepts and procedures should be simple enough that
handling datasets should be at least as simple as handling individual
files -- otherwise we might as well handle just files!

FIXME: Seen from operations and reported bugs: current work models are
complex and are to be simplified.  Constructing datasets error prone.
Reworking entire system, including framework, production tools, etc.,
not to enshrine current stuff as the data management system.
Proactively make sure end-to-end system makes sense, not building
complexity from the outside.

FIXME: Migration policy of current data to the new EDM (old data is
incompatible with the new EDM).  Similar issues will arise in future.
Accessing legacy systems for legacy data.  End-of-life policy.

FIXME: File naming conventions.  Especially if we use hierarchical LFN
space.  File merging consequences on file names.  Avoid putting too
much info in the name, names have to be of limited length.

FIXME: Blocks and data tiers.  Blocks of data for different tiers
should match, otherwise very hard to tell if the data available at a
site is really useful.  Or blocks should match runs?  But then, if all
data tiers are always in one file, this is moot.

FIXME: publishing system.  currently pubdb, advertise in refdb.  needs
to be re-divided between phedex (block tracking), pubdb, workload
management needs.


** Parametre system

Detail: My understanding is that event format like RAW or AOD is given
by the parameter ID?

FIXME: relationship to current owner information
FIXME: Also stores all parameters used by each algorithm?

    - evolution from rcp
    - which parameters can be queried?  which matter to the
      definition of dataset?  "what information is important?"
    - evolution of parameter attributes (queried -> not queried)
       - schema tied to orca version?
    - more important problems: managing dataset privately produced
      to publish, not super versatile query system
    - management through central database; flat file with job
    - interface area with dm / edm

** Conditions data

Interaction with...

CMS estimates of calibration data.  Very large (up to 5% of raw)?
Should calibration data management happen exactly the same as
normal data management?  E.g. stored in files, shipped around
using normal transfer mechanisms?

** Data transfer system

Much like PhEDEx is now.  Will move to peer-to-peer, with T0/T1s
possibly sharing a database.  Needs to be trivial to install and
operate/maintain, 0.0N FTE.

Push/pull/stream modes.  Pure data transfer system, not a replica
catalogue, not rest of "workflow".  Scales by # of files in transfer.
Leverages data chunking at higher level -- but doesn't require a chunk
to exist anywhere in entirety.  Robust, internet-like design, no one
part can bring entire transfer network down.  Maintains abstractions
at different layers.  Should be able to saturate any hardware
(network, storage) thrown at it.  Operates completely asynchronously
from other data management systems, production, worker nodes, ...

Layers/modules:
1. Unreliable point-to-point file transfer: globus-url-copy, srmcp, ...
2. Reliable point-to-point (= single-hop) transfer, local resource management
3. Reliable routed (= multi-hop) transfer
4. Dataset/chunk-level data transfer
4.a. Allocate files to destinations, back-up routing etc.
4.b. Monitor transfers at chunk level, notify site on progress
4.c .Activate/deactivate chunks
5. Placing files into transfer
5.a. Production link: harvest files from completed jobs
5.b. Bulk transfer requests: pick transfer assignments

Timing of SRM availability at all sites?

** Validation

Avoid over-validation.  Hierarchical validation (validating X also
validates Y.)

Technical validation.  As close to production as possible.  Make file
integrity guarantees (checksum, size, ...).  EDM (/POOL/Root) tool to
verify file integrity.  Produce a summary file, no need to validate
this information further until published.  Does not verify data
quality.  Event bit handling is in other files.

Transfer validation.  Ensures file integrity.

[Currently have site-local validation. This is to be suppressed if at
all possible.  If we get the same files over to the other side, we
should have everything correct by definition.  Need to think how this
interacts with publishing.  If meta files are no longer needed and
transfer systems idea of available blocks == current pubdb
information, then there's little more to do here.]

Physics validation, including calibration validations.  Adds data
quality tags to dataset bookkeeping system.  Data is not removed
at this point, it is only tagged.  People will simply not use
data without the tags they require.

[Tony/Nicola have much more detailed division.]

Handling lost files.  How many other files need to be marked lost --
if all data is in one file, only one file is affected by lost file.
Luminosity etc. sections should map to individual files.

Where validation results are recorded.  DBS validation tags.
Interaction with local vs. global DBS -- can I add validation tags in
my own local replica of the global DBS?

Where is what done?  DM tracks validation, need to define workflows.
Who takes care of doing this.  Eventually a data quality system?  Is
this a separate database?  Like luminosity/conditions tracking system?

 validation shouldn't be tied to unit of dataset, but incremental
   while files are being produced and stage in
   as light-weight as possible
   as early as possible

 validation must also be possible at any time (disks lost, ...)
   try to come up with finely grained list of all possible checks,
    when what why how cost, how to record results
    overlapping validations (e.g. hits validated by digitisation)
      -> implications for analysis?

 additional validation for skimming, file merging

 validating the data management itself of the files
   (when bookkeeping doesn't match files)

 model for first-pass reconstruction, including calibration validations

 when importing files, must not need to go through every file



** File catalogues and access authorization

Two separate aspects here: catalogue client and implementation.
Client is POOL interface.  Implementation may be local look-up table
(no service) or site-local file catalogue based on relational
database.  Auxiliary catalogues (XML) may be used, either accessed
directly from storage or downloaded to worker node with http/gsiftp
(external connectivity not required).  The catalogue used to record
and look up replica paths, nothing else.

The catalogue is to be entirely *local*, but may contain several
replicas for local load balancing (is this in conflict with previous
point of logical name spaces?).  Catalogues are updated and accessed
by the data transfer system (update a no-op for lookup table).  We do
not use catalogues for cross-site replica discovery.

FIXME: in major practical cases like the Castor or dCache based
storage elements that functionality is basically provided by the SE
"names space" (PNFS in case of dCache). These systems are already
highly optimized and robust (or at least equally robust as the storage
itself), so we probably should directly use them. One way of doing
that is to use URL-style file names everywhere, which also (as far as
I understand) provides access control etc.

FIXME: Attractive to try to suppress catalogues altogether.  Jobs
would discover prefix configuration probably using the same mechanism
they get their software / other site-specific information for the jobs
(obviously not a separate file on every worker node).

FIXME: access control lists, authorization.  Role-based security based
on groups?  Can ACLs be implemented robustly at all at timescale of
baseline?  [FIXME: Ian Fisk's comments on role-based mapping to GIDs
in UID.GID standard POSIX security model.  Dynamically created groups
a tough problem.  Currently implemented in globus gatekeeper and
dcache.  can start with very coarse groups (uscms01.users)] Real ACLs
challenging, can do something for "co-operative systems" like dCache.

CMS data protected from other experiments.

** Storage management and data serving

Assume storage service with internal catalogue, i.e. logical naming
structure mostly separate from actual physical storage layout (= no
actual disk mount points etc.).  A site, in particular a very small
site (laptop :-), may expose physical layout, but takes on full
management responsibility then.  Recommended to use highly reliable
and scalable storage services (dCache, Castor, xrootd, lcg stuff,
...).

Say something about SRM?  We expect everyone to operate SRM, including
transfers, space management, advisory delete, etc.  How much focus
needs to be put on reliable, deployable non-dCache SRM?

We don't do tape management, it's supposed to be hidden by SRM,
however allowing necessary efficient management / hinting.

Tactical vs. strategic storage.  Resource quotas.  Etc.

** Data management scopes

Local vs. global data management.  Means two separate things: a)
site-local data management vs. one done globally within CMS, and b)
data management done by individuals, groups and CMS as a whole, with
different ranges of data scope/visibility.

Division of work between CMS managing its own data, and system
operator management.  CMS manages the data logically; operators only
manage storage and file placement in pools etc. (SRM, SE and below).
Operators never need to manage CMS data logically; CMS manages space
etc. reservations with SRM-like tools.  Catalogues etc. are entirely
CMS domain.  CMS provides means to release file space when files are
no longer needed, e.g. after they have been made safe in Tier-1
custody.  CMS provides tools for (CMS) site data administrators.

Data management done by individuals, groups etc.  Including user
private data management -- same tools as everywhere else.  Production
also uses the same tools, just different scale.  Publishing data to
other individuals.  Using DBS functions/services locally in groups
working locally (e.g. in Tier-2), then later making the data available
publicly.  Propagating info up and down the chain: global, group,
local data management.  Basic line: same tools used everywhere, can be
hooked up together (e.g. can have local dataset bookkeeping system
which tracks own private datasets + mirrors information from cms-wide
info + mirrors local group's dataset bookkeeping as well, and all this
can later be published into the central database).  Scope of the data,
who sees data from any particular dataset bookkeeping system instance.

File custody handling.

User private file management (ntuples etc.).  Basically the same as
any other data we track and move about.

Moving vs. replicating files.  When files have made it to the
destination, remove original.  Can already be done with cleaner-type
agents.

** File and buffer management

Sorting events, luminosity.

How does data get out from online farm, buffer management.

MC data (pileup in one files, hit files, ... -> full event files).

File merging.

** Priorities and policies

Use private data management, local management.  Small MC jobs, ntuple
management.  Important use case, especially ntuple backup,
distribution to jobs for grids.  (Idea: treat output from job family
as a single data block, gravitate it all to a destination specified by
user.  Remove at original source (run cleaner) when made it to
destination.  Allow back-up transfer assignment to Tier-1 for backup.
When private data required as job input, create assignments of the
block(s) to destination nodes.  Job can be started when data is
reported as moved.  So this works exactly like the CMS-wide scale.)

"Outer rim" for production use.  There's an edge to the world where
CMS can run services.  Jobs can run outside that, and outside can be
represented as "virtual" nodes (e.g. USMOP), but files in catalogue
are arranged to be gsiftp://... from some "last edge of CMS", and are
downloaded to the worker node on the fly -- not placed/pushed to the
site before job, but fetched to tactical storage on the fly, then
removed after the job.

File naming conventions.  LFN space usage.

Data management in online environment?

Workflows etc.

Priorities as fallback solutions.

** Other stuff

 how to update ddd with alignment?


**********************************************************************
* Workload management

Sharing tools for production, analysis, private productions, ...
One set should be able to cope with all the cases.  Mindset:
production should be able to use tools for analysis.

Using different grids and batch systems.  Local vs. global
submission.

Generating chains of jobs (cmkin, oscar, orca, dst, ntuple).

Job splitting.

User specification

Gets data atoms from data management (self-consistent units of data
with some unique key, suitable for scalable and manageable
partitioning of jobs), negotiates job splitting.

WM needs to discover from dm where data is, then tell framework what
data to process and how to access it at the site.  As baseline will go
to dataset bookkeeping system, then dataset location service, submit
to resource broker / batch queue, jobs land on site, discover from
site configuration where the data is (local catalogue and/or rules
to create PFNs); no site-specific information will go with the job,
it is all discovered at the site.

In future resource broker may negotiate job splitting with dm (data
location service in particular), but this is more complex than "just
get list of locations" -- two-way negotiation of offering chunks,
offering job split, getting available location list. Not however
baseline system.

What needs to go with the job: meta data (event directory?) from
dataset bookkeeping system, other dm-provided meta data, local
information (catalogue, PFNs).

Job splitting can happen at user level, wm level, mixed; user may give
only constraints (e.g. job run time, max amount of runs to process per
job?).  Splitting is a set of jobs with the same requirements =>
cluster that supports bulk operations (submit, query, status, cancel,
...) + access to individual jobs.  subjob number available at wm
level.  Authentication handshake.

UI-level splitting is what we do now and will be the baseline system.
Done by user, not data distribution, divided to job run-time etc.
Typically need to provide whole dataset; don't see user-level
splitting.

RB-level splitting; not possible with current brokers.  User defines
constraints (limit on # of jobs).  Need mechanism to negotiate split
with data management and resource broker.

In all cases, but especially in RB-level split, one significant issue
is mapping job split into parameters to be passed to cobra: input
collection and event range specification.

Basic requirement for resource broker is that of a queue system.
Baseline is: i want to run these sets of jobs on sites compatible
with these constraints.  Resource broker makes no decisions or
optimisation.

Jobs discover local configuration when they land on site.  Part of
VO-specific site configuration (source some environment script).
Includes local catalogue etc. contacts.  Available to the jobs the
same way as e.g. software is (shared file system typically).

Workload management chooses minimal set of files to process.
Finer grainer selection done by application, including use
of event directory.

Data block replica location and data quality attributes are
separate sub-services.

*** Job monitoring

*** Other stuff

in mcrunjob
 - module (evdservices) service to build datasets before ...
    - get assignment id -> query redb -> get list of files, parameters
    - transfer those files to your local system (outside evdservices)
    - run attachrun, fixcoll, ...
    - quick and dirty, originally written by julia
    - refactor, split out of mcrunjob, more general purpose tool
        -> publish service
        - also used at fnal to build datasets published in pubdb
        - uses other services to do the actual publishing
    - mission: prepare datasets for subsequent processing
       once files are available, whatever it takes to make those
       files together usable for analysis (could be largely suppressed
       with suitable changes to edm)

 - get parameters from refdb for jobs; refdb does job splitting,
   mcrunjob does not decide it - gets job splitting files, iterate
   over, create jobs
    - job definition constraints: how large my output files are,
      how long do i need to run, how many worker nodes are available,
      ... => quickly very complex decision, choose most important
      criterions -> data management criteria prevail
    - little in common with crab, overlapping functionality
    - discussion on integration of crab/mcrunjob/mcps?  not yet,
      should come from aprom

 - summary procedure
    - template scripts for jobs are also kept in refdb
    - contain commands to generate summary information
    - re-engineering proposal: mcrunjob high-level service
    - does: filters job stdout + uses some of assignment parameters
       - looks for carf resume run netlogger output; pointer to
         some important information to be used by attachrun
       - also cross section (error messages? not...)
       - now also file checksums, sizes, etc.
       - delivered to refdb with sendmail; sites that can't
         handle this send them by hand in batch (not scalable)
         maybe use agent type functionality to deliver the summary?
    - future considerations: refdb re-engineering (dm to be
      part of)

 - files are stored on local site in mass storage
    - files created on local disk of the worker node
    - site-specific job-creation-time to copy data off worker
      nodes to storage (stageing script)
    - reading could occur directly from mass storage, or copied in
    - now creating a zip file of all the files (lcg only for now)
    - unspoken assumption: all data in us goes to fnal, other to
      infn or cern (usmop -> fnal, follow convention to locate)
    - have capability to stage the files out wherever wanted
    - file removal after transfer to be handled

 - considering putting in validation tools, holding for discussion
   with data management group, nicola, within production group

 - next version will have whole new set of tools
    - timescales: new technology integration 8-12 months
       current version frozen for physics tdr, once completed, rolling
       technologies are available now, continuing development
    - xml configuration of jobs
    - better interface to batch systems
    - better runtime instrumentation

 - dataset bookkeeping service: how do you get .oracrc parameters
   to the job; if data movement needs to occur, how does that get
   invoked; prestaging hooks
    - MS1. dataset discovery; queries + presentation (maybe physh)
    - MS2. data block matching sites / workload management (crab)
    - MS3. registration dataset bookkeeping system
      - plugging into summary procedure
      - harvesting files from storage elements, taking file ownership
        and responsibility, moving vs. transferring
      - active integration with phedex
    - MSN. technical validation

    - two types of dataset discovery: what to run on
       data available, number of events, etc.

    - what is dataset bookkeeping service
      - metadata about dataset entities
      - create, populate, discover datasets, resolve for files
    - use cases
      - create/define dataset
         - in case of mc: request approval
         - real/production dataset: stream
         - different scopes: experiment-wide, smaller scopes
         - in: attributes; out: id
      - populate: add (lfn, attrs, dsid)
      - close dataset
      - import: create + populate in single go
      - browse/search: in: attributes; out: dsids of summary
    - how do you manage open blocks (or open datasets)
      - system + users need to know dataset (block) is open
      - need to be clear how we define our terms
      - sample / run range / quality attributes => will run over
      - authoritative source/site for blocks
      - site offers to serve certain blocks (and can go get on demand)
      - will open blocks be published?  no?  real latencies in system?
      - cvs-type tags on growing datasets (independent of blocks)
          - tag for data range as files/run range -> conference data
      - available units of division: what job can run on,
          what site will host
      - intermediate data management unit

   - "i want to analyse this run range in data set x"
      - user: how many events, how much luminosity, how much data touched
      - tool: how can i use the same info to split into jobs, with time
        constraints etc. -> N jobs that run at RAL, M at FNAL
   - luminosity integrated at appropriate levels
      - blocks are data management, not physics unit
      - luminosity stored in separate database, dm references to segments,
        approximate rough estimate in dataset bookkeeping system
   - when importing data for certain run range
      -> may map to inconvenient set of blocks ("laptop use case")
      -> blocks should map to usage patterns, may be reshuffled
   - organise mini-workshop with emilio: do files need to be resorted
      on output from online system, do events need to be ordered, periods
      of stable running conditions; go to run meetings for magnet meetings
      (by austin ball) -> ask for task force to consider these issues
   - owner-related information needs to be stored somehow
      group owner, pileup, transformation, ...

   - proposed implementation
      - relational dbms backend (not tied to specific database)
      - three principal schema components
          - dataset-level meta data
          - file-level meta data
          - file membership in datasets
      - client communication with web-enabled db server through
         higher-level interface (not db schema)

   - scenarios:
      - production manager assigns jobs to run at some site B, but
        input at site A, invokes some interface to make the input
        dataset available at site B

to be considered
 - file merging
 - 

** Software distribution

DAR vs. RPMs vs. Pacman.  Installing software at sites.  Special
production needs?  Interaction with other aspects?  Use one mechanism
for distributing files to sites?

**********************************************************************
* Current CMS data management system

The functionality of the data management is currently achieved through
a number of components and services that are fairly loosely coupled.
While not exactly the data management system we expect to have on LHC
turn-on, it does include a number of design principles we intend to
maintain.

The core of the dataset bookkeeping system is formed by the virtual
data view and logical file catalogue parts of RefDB.  Instances of
PubDB extend this for site-local information.  [FIXME: Dataset
definitions: dataset, owner, collection?.  Various aspects of the
virtual data view and logical file catalogue in RefDB.  Exactly what
PubDB provides today.]

The data transfers are implemented using PhEDEx.  [FIXME: Expand a
little, layering from below.]

Each site has local POOL file catalogues that has several
functions. The set of files that belong to the datasets stored at a
given sites are implcitly defined through the POOL pointer structure
-- all files in a dataset will "by magic" be contained in the dataset
POOL catalog, which also specifies the physical location of files
pointed to by event references in the collection.

There is no global replica 
catalogue in CMS.  Catalogue consistency is maintained between sites
as a part of the data transfer system.  In addition various
consistency checking tools are provided -- what PhEDEx thinks the site
has, what's on storage element (disk, tape), what's in local file
catalogue, what's been published in PubDB, what files RefDB knows
about.  The site-local catalogue for event data files is a RDBMS
(MySQL, Oracle) POOL file catalogue.  XML catalogues are used for
other files.

Storage systems: LCG SE, dCache, Castor, SRM/dCache, raw disk pools.

User interface: PHYSH, production web pages, command line tools.

**********************************************************************
* Organisation

** Projects

Champions + members.  Broken down to fairly small projects, any one
person might champion two projects and be a member in three or four
others.  Champion responsible for reporting, calling meetings, etc.
Otherwise everybody works in peer structure.  Some projects might be
short-lived, others might be longer-lived.

** Schedule

Built from components proven before then.

Preceding system test milestones.

Spreading data challenge into integration testbeds, DC06 dedicated
focusing point.

FIXME: Expanded version of the bits about scheduling in baseline
description.

FIXME: I think we should describe the processes of development of new
components that get then integrated and eventually moved into
production. There is an integration testbed and a set of services
challenges that prove a new revision of services and functionalities,
performed in an integration environment, that then can be released for
production when certified. etc pp

FIXME: Interactions of new EDM and new dataset bookkeeping system.
Try to do in the new way where possible.  Only keep separate what
required, will consider at that time how to do the fork.  Demonstrate
upgrades.

*** Intermediate goals (now, six months, baseline)

Now.  Summer.  Magnet test ~ Oct/Nov.  DC06.  End of 2006.  April 2007.

Baseline in place for DC06.

Integration testbed plans.

What needs to be done immediately

Development process, who does what.

milestones
  baseline always working
  system tests; in beginning not all compoennts, inreasingly
    more components into testbed, when work, moved into production

 prs milestone to deliver condition details / volumes / latencies / rates?


** Operation

Migration policy.  New EDM.

**********************************************************************
* Use cases, scenarios, requirements, workflows

scenarios for data taking, simulation production, processing
production, physics analysis

scenarios for production-level creation and manipulation of datasets
ditto for physics user-level cration and manipulation of datasets

scenarios of physics user-driven movement of datasets to T2s, laptops
-- including deployment scenarios

Import
Queries

Workflows
(validation) data management system independent of data quality
 - technical validation vs. physics validation
 - dms tracks separately bad (corrupted/lost/...) data and
    physics attributes
 - do we need workflow hooks for initiating physics validation?
