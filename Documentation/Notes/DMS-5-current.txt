**********************************************************************
* Current CMS data management system

The functionality of the data management is currently achieved through
a number of components and services that are fairly loosely coupled.
While not exactly the data management system we expect to have on LHC
turn-on, it does include a number of design principles we intend to
maintain.

The core of the dataset bookkeeping system is formed by the virtual
data view and logical file catalogue parts of RefDB.  Instances of
PubDB extend this for site-local information.  [FIXME: Dataset
definitions: dataset, owner, collection?.  Various aspects of the
virtual data view and logical file catalogue in RefDB.  Exactly what
PubDB provides today.]

The data transfers are implemented using PhEDEx.  [FIXME: Expand a
little, layering from below.]

Each site has local POOL file catalogues that has several
functions. The set of files that belong to the datasets stored at a
given sites are implcitly defined through the POOL pointer structure
-- all files in a dataset will "by magic" be contained in the dataset
POOL catalog, which also specifies the physical location of files
pointed to by event references in the collection.

There is no global replica 
catalogue in CMS.  Catalogue consistency is maintained between sites
as a part of the data transfer system.  In addition various
consistency checking tools are provided -- what PhEDEx thinks the site
has, what's on storage element (disk, tape), what's in local file
catalogue, what's been published in PubDB, what files RefDB knows
about.  The site-local catalogue for event data files is a RDBMS
(MySQL, Oracle) POOL file catalogue.  XML catalogues are used for
other files.

Storage systems: LCG SE, dCache, Castor, SRM/dCache, raw disk pools.

User interface: PHYSH, production web pages, command line tools.
