**********************************************************************
* Development strategy

The baseline data management system design incorporates requirements
from use cases (see separate chapter) and our experience of what will
reasonably work at large scale.  That is, we apply our experience to
deliver a data management system which gives CMS a competitive edge by
providing reliable, adaptable and scalable access to data.  This
chapter outlines the strategic choices we believe to be essential for
this.

New components proposed for inclusion into the data management system
must have proven record of reliability and scalability in real-life
integration tests.  In other words technology must be deemed both
desirable and validated to be considered for inclusion.  We believe we
have all the technical knowledge to implement the baseline system:
either the components already exist, or we have a clear idea of how to
design and develop them.  Hence we do not expect research to play a
significant role in delivering the baseline data management system.
The most important work of the project is to deliver a verified
operational environment.

The baseline system will be developed by evolving and refactoring
existing services.  This will ensure for one that there is always a
working system, and for other teaches us how to develop a deployed
system including planning and execution of version upgrades.

[FIXME: backward/forward compatibility?]

[FIXME: loosely coupled set of services and components.]

** Strategical choices

We conclude the following from our experience:

1) We have demonstrated ability to run high-availability local
   services in few areas: disk servers, mail system, databases,
   limited number of a web services.  Few centres are able to provide
   such services with high reliability: this is often proportional to
   manpower available to ensure smooth operation.

2) We have demonstrated ability to deliver high-availability non-local
   services only in very few areas: internet infrastructure services
   (mail, dns), web to some extent.

3) It takes at least a year to go from concept to delivering reliable
   service when *underlying design was sound to begin with.* It takes
   considerably longer if one has to iterate to get to sound design.

4) [FIXME: Need to migrate systems in a coordinated manner -- taking
    people and systems ahead, but not breaking previous generation
    unnecessarily.  Issues with migrating experiment as a whole
    through both software and system generations: it's not that
    different whether it is reconstruction software generation,
    or migrating our system services.  Tension among inability to
    run distributed services (logic in service), client proliferation
    (logic in clients), service upgrades (logic in intermediate
    service libraries/toolkits/...).  State clear conclusions that
    can be used to derive guidance.]

We have accounted for these by describing a baseline system that is
conservative regarding these factors, but partitioned such that we
retain the option to exploit more advanced services in future as they
mature.

When considering how services ought to be distributed, we select as
design guidance a) required degree of synchronisation, b) sensitivity
to failures, and c) intensity of operations (frequency, bandwidth),
and d) the degree to which two ends of the communication must agree on
protocol.  No service should be designed to rank highly in all four
aspects.  The lower the ranks, the more reliable, scalable and
maintainable the system: the better it is capable of delivering
physics.  The higher the rank in any area, the closer the service
ought to be to the client, or the harder it is to upgrade systems.

For example:

1) Selecting data to run on requires low level of synchronisation (=~
   correct within hours or days).  It's sensitive to failures in that
   the job won't get done -- but the human can always come back later.
   Intensity is low.  This could be handled by central service,
   distribution would add robustness in adding fallbacks.

2) Any service accessed by worker node job is highly sensitive in all
   three areas.  Therefore we should minimise such services, or design
   them in ways to lower the ranks.  For example where database
   service reliability is bound to be low, caching-based solutions are
   better (asynchronous caching is even better).  Data I/O and file
   name lookup require services that at least as reliable as the
   underlying storage service -- a good reason to unify the two.

3) Collecting job results can be made less risky by doing it
   separately from the job itself where that is an option, failing
   that by making fewest possible connections to most reliable
   possible services from the job.

Where services need to be brought close to the client, we need guides
for sound choices.  The main approaches we consider to be scalable
and reliable:

1) Keep operated systems simple.  Eliminate services that are not
   required, and share technology for different services where it
   can be done sensibly.  Actual exploited systems should not vary
   unnecessarily.

2) Include only what is truly needed.  The existence of every
   component and service must be justifiable in the context of the
   entire end-to-end system, accounting both advantages as well as
   risks introduced by the component.

3) Partition the problem.  [FIXME: Address authorative information
   sources.]

3.a) If data is local, keep it local.  If it's purely local, don't
  expose it outside.  If access is required from outside, it may
  simply be a whole lot more expensive to get at it.

3.b) If the data can be layered or sliced, do so.  This allows
  different segments to be addressed differently, and even to be
  ignored.  For instance higher-level data may be replicated
  everywhere asynchronously, and only details about "interesting bits"
  stored locally.  Combination with 2.a makes things even better.

4) Cache; update via separate channel if necessary.  Frontier and
   using web cache-type technology.

5) Peer-to-peer distribution.  May or may not have a central master.
   These are similar to core internet services.  Cost to access
   information or available detail may vary with distance to the
   source.

6) Encapsulating "business logic" either in servers, or bundled
   libraries; thin clients; protocol conformance.  Versus auto-update
   technology.

[FIXME: Simplify where current model / workflows are complex.  Don't
build a model that capitalises on manpower and diligence of people.
Avoid error-prone procedures.]
