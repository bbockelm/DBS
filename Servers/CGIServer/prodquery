#!/usr/bin/env perl

######################################################################
# This is the CGI-based server for querying the DBS database.  Queries
# are made as HTTP GET requests.  Responses come as text/plain result,
# with some additional DBS-specific reply headers.  Usually the result
# is in XML.  See the PythonAPI for the client side of this exchange.
#
# The server is hosted on CERN web servers and accesses the DBS Oracle
# database hosted at CERN.
#
# This server script requires the following additional components:
#  - Database parameter file which defines the database contact and
#    authentication parameters.  This file is stored in AFS directory
#    only accessible to DBS admins and the web server itself.  This
#    is currently /afs/cern.ch/cms/aprom/DBS/DBAccessInfo/DBParam.
#  - PhEDEx toolkit modules, which currently implement DBS access.
#    These are currently in /afs/cern.ch/cms/aprom/phedex/PHEDEX.
#  - Extra perl modules and the Oracle instant client libraries.
#    These are currently in /afs/cern.ch/cms/aprom/phedex/Tools.
#   
# The details about the query are returned in three extra HTTP reply
# headers: dbs-status-{code,message,detail}, plus the reply body.
# Of these, dbs-status-code and -message are always given; the former
# is a numeric code and the latter a textual version.  The third
# header dbs-status-detail may be present in errors and may explain
# in more detail what went wrong.  The status codes are as follows:
#
#  100      Success, reply body contains result
#  200-299  The request itself was not accepted by the server.
#  300-399  The parameters supplied in the request were not accepted.
#  400-499  The request was well-formed, but a runtime error occurred.
#  500-599  Requested data does not exist.

######################################################################
# Prepare the environment.  We can't source the environment setup
# scripts since they are for bourne shell.  Just hardcode minimal set.

BEGIN {
  if (! $ENV{MOD_PERL})
  {
    use strict; use warnings; $^W=1; use Config;
    die "\$TNS_ADMIN not set\n" if ! $ENV{TNS_ADMIN};
    die "\$ORACLE_HOME not set\n" if ! $ENV{ORACLE_HOME};

    unshift(@INC, "/data/DBSPerlLib/lib");
    unshift(@INC, "/data/DBSPerlLib/lib/$Config{archname}");
  } 
}

use CGI qw(header param);
use DBI;

######################################################################
# Main program

my $dbparams = "/data/DBSAccessInfo/DBParam";

# Known API calls
my %apis = (
  'getDatasetProvenance' => [ \&getDatasetProvenance, {
    api => REQUIRED, path => REQUIRED, datatier => OPTIONAL } ],
  'getDatasetContents' => [ \&getDatasetContents, {
    api => REQUIRED, path => REQUIRED } ],
  'getDatasetFiles' => [ \&getDatasetFiles, {
    api => REQUIRED, path => REQUIRED } ],
  'listDatasets' => [ \&listDatasets, {
    api => REQUIRED, pattern => OPTIONAL } ]);

# Get parameters
my $api = param('api');
&reply_failure (200, "Bad request", "API call was not defined")
  if (! defined $api);
&reply_failure (300, "Bad data", "Requested API call was not recognised")
  if (! exists $apis{$api});

&checkParameters ($apis{$api}[1]);
&{$apis{$api}[0]} ();

######################################################################
# Common routines.

# Make sure the call is valid.  Make sure all required arguments are
# present, and reject any excess arguments.
sub checkParameters
{
  my ($params) = @_;
  my @excess = grep (! exists $$params{$_}, param());
  my @missing = grep ($$params{$_}{REQUIRED} && ! defined param($_), keys %$params);
  if (@excess || @missing)
  {
    my $err = "";
    $err .= "Excess parameters @excess\n" if @excess;
    $err .= "Missing required parameters @missing\n" if @missing;
    &reply_failure (200, "Bad request", $err);
  }
}

# Validate a path.
sub getPath
{
  my $path = param('path');
  &reply_failure (200, "Bad request", "No path specified")
    if (! defined $path);
  &reply_failure (300, "Bad data", "Unsafe characters in path")
    if ($path !~ m|^[-A-Za-z0-9_./]+$|);
  &reply_failure (300, "Bad data", "Expected path of the form DATASET/TIER/OWNER")
    if ($path !~ m|^/([^/]+)/([^/]+)/([^/]+)$|);

  return ($1, $2, $3);
}

# Connect to the DBS database.
sub connectToDBS
{
  my $self = { DBCONFIG => "$dbparams:Production/Reader" };
  my $dbh = eval { &connectToDatabase ($self) };
  &reply_failure (400, "Database connection failure", $@) if $@;
  &reply_failure (401, "Failed to connect to DBS", "") if ! $dbh;
  $SIG{INT} = 'DEFAULT'; # Restore ORACLE's swallowed signals.
  return $self;
}

# Print standard response headers.  We always print two extra response
# headers, "dbs-status-code:" and "dbs-status-message:", the former a
# numeric value and the latter the same in clear language.  In case of
# errors we may also add "dbs-status-detail:" header to expand on the
# cause of the error.
sub response_headers
{
  my ($code, $msg, $detail) = @_;
  my %args = (-dbs_status_code => $code,
  	      -dbs_status_message => $msg);
  if (defined $detail)
  {
    $detail =~ s/\n/ /sg;
    $args{dbs_status_detail} = $detail;
  }
  print header (-type => 'text/plain', %args);
}

# Generate a failure reply.
sub reply_failure
{
  my ($code, $msg, $detail, @info) = @_;
  unlink (@tmpfiles);
  &response_headers ($code, $msg, $detail);
  print @info;
  exit (1);

}

# Generate a success reply.
sub reply_success
{
  my (@data) = @_;
  unlink (@tmpfiles);
  &response_headers (100, "Success", undef);
  print @data;
  exit (0);
}

######################################################################
# Parse database connection arguments.
sub parseDatabaseInfo
{
    my ($self) = @_;

    $self->{DBH_LIFE} = 86400;
    $self->{DBH_AGE} = 0;
    if ($self->{DBCONFIG} =~ /(.*):(.*)/)
    {
	$self->{DBCONFIG} = $1;
	$self->{DBSECTION} = $2;
    }

    my $insection = $self->{DBSECTION} ? 0 : 1;
    open (DBCONF, "< $self->{DBCONFIG}")
	or die "$self->{DBCONFIG}: $!\n";

    while (<DBCONF>)
    {
	chomp; s/#.*//; s/^\s+//; s/\s+$//; s/\s+/ /g; next if /^$/;
	if (/^Section (\S+)$/) {
	    $insection = ($1 eq $self->{DBSECTION});
	} elsif (/^Interface (\S+)$/) {
	    $self->{DBH_DBITYPE} = $1 if $insection;
	} elsif (/^Database (\S+)$/) {
	    $self->{DBH_DBNAME} = $1 if $insection;
	} elsif (/^AuthDBUsername (\S+)$/) {
	    $self->{DBH_DBUSER} = $1 if $insection;
	} elsif (/^AuthDBPassword (\S+)$/) {
	    $self->{DBH_DBPASS} = $1 if $insection;
	} elsif (/^AuthRole (\S+)$/) {
	    $self->{DBH_DBROLE} = $1 if $insection;
	} elsif (/^AuthRolePassword (\S+)$/) {
	    $self->{DBH_DBROLE_PASS} = $1 if $insection;
	} elsif (/^ConnectionLife (\d+)$/) {
	    $self->{DBH_LIFE} = $1 if $insection;
	    $self->{DBH_CACHE} = 0 if $insection && $1 == 0;
	} elsif (/^LogConnection (on|off)$/) {
	    $self->{DBH_LOGGING} = ($1 eq 'on') if $insection;
	} elsif (/^LogSQL (on|off)$/) {
	    $ENV{PHEDEX_LOG_SQL} = ($1 eq 'on') if $insection;
	} else {
	    die "$self->{DBCONFIG}: $.: Unrecognised line\n";
	}
    }
    close (DBCONF);

    die "$self->{DBCONFIG}: database parameters not found\n"
	if (! $self->{DBH_DBITYPE} || ! $self->{DBH_DBNAME}
	    || ! $self->{DBH_DBUSER} || ! $self->{DBH_DBPASS});

    die "$self->{DBCONFIG}: role specified without username or password\n"
	if ($self->{DBH_DBROLE} && ! $self->{DBH_DBROLE_PASS});
}

# Create a connection to the database.
sub connectToDatabase
{
    my ($self, $identify) = @_;

    # If we have database configuration file, read it
    &parseDatabaseInfo ($self) if ($self->{DBCONFIG} && ! $self->{DBH_DBNAME});

    # Start a new connection.
    $dbh = DBI->connect ("DBI:$self->{DBH_DBITYPE}:$self->{DBH_DBNAME}",
	    		  $self->{DBH_DBUSER}, $self->{DBH_DBPASS},
			  { RaiseError => 1, AutoCommit => 0, PrintError => 0 });
    return undef if ! $dbh;

    # Acquire role if one was specified.  Do not use &dbexec() here
    # as it will expose the password used in the logs.
    if ($self->{DBH_DBROLE})
    {
	eval { $dbh->do ("set role $self->{DBH_DBROLE} identified by"
		         . " $self->{DBH_DBROLE_PASS}") };
	die "failed to authenticate to $self->{DBH_DBNAME} as"
	    . " $self->{DBH_DBUSER} using role $self->{DBH_DBROLE}\n"
	    if $@;
    }

    # Cache it and set some important parameters.
    $self->{DBH} = $dbh;
    $self->{DBH}{FetchHashKeyName} = "NAME_uc";
    $self->{DBH}{LongReadLen} = 4096;
    $self->{DBH}{RowCacheSize} = 10000;
    return $dbh;
}

# Tidy up SQL statement
sub dbsql
{
    my ($sql) = @_;
    $sql =~ s/--.*//mg;
    $sql =~ s/^\s+//mg;
    $sql =~ s/\s+$//mg;
    $sql =~ s/\n/ /g;
    return $sql;
}

# Simple utility to prepare a SQL statement
sub dbprep
{
    my ($dbh, $sql) = @_;
    return $dbh->prepare (&dbsql ($sql));
}

# Simple utility to prepare, bind and execute a SQL statement.
sub dbexec
{
    my ($dbh, $sql, %params) = @_;
    my $stmt = &dbprep ($dbh, $sql);
    my $rv = &dbbindexec ($stmt, %params);
    return wantarray ? ($stmt, $rv) : $stmt;
}

# Simple bind and execute a SQL statement.
sub dbbindexec
{
    my ($stmt, %params) = @_;

    while (my ($param, $val) = each %params) {
	$stmt->bind_param ($param, $val);
    }

    return $stmt->execute();
}
######################################################################
######################################################################
######################################################################
# Actual API implementation routines.  We code many of the queries for
# better efficiency directly against the schema as UtilsDBS isn't well
# geared for our purposes right now.

######################################################################
# List available (processed) datasets.
sub listDatasets
{
  use Text::Glob 'glob_to_regex';
  $Text::Glob::strict_wildcard_slash = 0;

  # If a pattern was given, make sure it's a valid one and convert to
  # a perl regular expression.  The patterns are shell globs.
  my $pattern = param('pattern');
  my $rxpattern = undef;
  if (defined $pattern && $pattern ne '')
  {
    eval { $rxpattern = &glob_to_regex ($pattern) };
    &reply_failure (300, "Bad data", "Invalid match pattern") if $@;
  }

  # Fetch the list of datasets from the database.
  my $dbs;
  eval
  {
    $dbs = &connectToDBS ();
    my $q = &dbexec($$dbs{DBH}, qq{
      select
        procds.id,
        primds.name,
        procds.name,
        dt.name
      from t_processed_dataset procds
        join t_primary_dataset primds
          on primds.id = procds.primary_dataset
        join t_processing_path ppath
          on ppath.id = procds.processing_path
        join t_data_tier dt
          on dt.id = ppath.data_tier});

    my $out = "<?xml version='1.0' standalone='yes'?>\n<dbs>\n";
    while (my ($id, $primary, $processed, $tier) = $q->fetchrow())
    {
      my $token = "/$primary/$tier/$processed";
      next if (defined $rxpattern && $token !~ m/$rxpattern/);
      $out .= "  <processed-dataset id='$id' path='$token'/>\n";
    }
    $out .= "</dbs>\n";

    # Spit out the result
    $$dbs{DBH}->disconnect ();
    &reply_success ($out);
  };

  # If there was a failure, spit out an error.  Note that "exit" is a
  # special "die" call under mod_perl, so handle it gracefully.
  exit if $@ && ref $@ eq 'APR::Error' && $@ == ModPerl::EXIT;
  if ($@) { $@ =~ s/\n$//s; &reply_failure (402, "Execution error", $@) }
  eval { $$dbs{DBH}->disconnect () if $$dbs{DBH} };
}

######################################################################
# Get dataset provenance.
sub getDatasetProvenance
{
  # Parse parameters.  If 'datatier' is given, supply only ansers
  # matching those datatiers (comma-separated list of data tiers).
  my @path = &getPath ();
  my $parentspec = param('datatier');
  &reply_failure (300, "Bad data", "Unsafe characters in datatier")
    if (defined $parentspec && $parentspec !~ m|^[A-Za-z,]+$|);

  # Fetch dataset provenance.
  my $dbs;
  eval
  {
    # Connect to the database.
    $dbs = &connectToDBS ();

    # Verify all requested datatiers are known
    fetchAll ($dbs, "data_tier");
    fetchAll ($dbs, "parentage_type");
    my @parents = (defined $parentspec ? split(",", $parentspec) : ());
    foreach my $t (@parents)
    {
      if (! grep ($_->{NAME} eq $t, @{$$dbs{PARENTAGE_TYPE}}))
      {
	$$dbs{DBH}->disconnect ();
	&reply_failure (301, "Bad parentage", "Parentage type '$t' not known");
      }
    }

    # Translate the dataset path to an id
    my $id = &datasetFromPath ($dbs, @path);

    # Fetch provenance info
    my $qdsinputs = &dbexec ($$dbs{DBH}, qq{
	select distinct
	    pt.name,
	    procds.id,
	    ppath.data_tier,
	    primds.name,
	    procds.name
	from t_event_collection ec
	join t_evcoll_parentage ep
	  on ep.child = ec.id
	join t_event_collection ec2
	  on ec2.id = ep.parent
	join t_processed_dataset procds
	  on procds.id = ec2.processed_dataset
	join t_processing_path ppath
	  on ppath.id = procds.processing_path
	join t_primary_dataset primds
	  on primds.id = procds.primary_dataset
	join t_parentage_type pt
	  on pt.id = ep.type
	where ec.processed_dataset = :id},
	":id" => $id);

    my $out = "<?xml version='1.0' standalone='yes'?>\n<dbs>\n";
    $out .= "  <processed-dataset id='$id' path='/$path[0]/$path[1]/$path[2]'>\n";
    while (my ($type, $id, $tier, $primary, $processed) = $qdsinputs->fetchrow())
    {
      next if defined $parentspec && ! grep ($type eq $_, @parents);
      $tier = (grep($_->{ID} eq $tier, @{$$dbs{DATA_TIER}}))[0]->{NAME};
      $out .= "    <parent path='/$primary/$tier/$processed' tier='$tier' type='$type' id='$id'/>\n";
    }
    $out .= "  </processed-dataset>\n";
    $out .= "</dbs>\n";

    # Spit out the result
    $$dbs{DBH}->disconnect ();
    &reply_success ($out);
  };

  # If there was a failure, spit out an error.  Note that "exit" is a
  # special "die" call under mod_perl, so handle it gracefully.
  exit if $@ && ref $@ eq 'APR::Error' && $@ == ModPerl::EXIT;
  if ($@) { $@ =~ s/\n$//s; &reply_failure (402, "Execution error", $@) }
  eval { $$dbs{DBH}->disconnect () if $$dbs{DBH} };
}

######################################################################
# Get the contents of the dataset: the blocks and event collections.
sub getDatasetContents
{
  # Parse parameters
  my @path = &getPath ();

  # Fetch the contents
  my $dbs;
  eval
  {
    # Connect to the database.
    $dbs = &connectToDBS ();

    # Translate the dataset path to an id
    my $id = &datasetFromPath ($dbs, @path);

    # Prepare output.
    my $out = "<?xml version='1.0' standalone='yes'?>\n<dbs>\n";
    my $pathname = "/$path[0]/$path[1]/$path[2]";
    $out .= "  <processed-dataset id='$id' path='$pathname'>\n";

    # Fetch blocks, event collections and files
    my ($blocks, $evcolls, $files) = &datasetContents ($dbs, $id);
    foreach my $block (@$blocks)
    {
      $out .= "    <block id='$$block{ID}' name='$pathname#$$block{ID}'>\n";
      foreach my $evc (sort { $$a{NAME} cmp $$b{NAME} } values %{$$block{EVCS}})
      {
	next if $$evc{NAME} eq 'EvC_META';
	my $name = $$evc{NAME}; $name =~ s/^EvC_Run//;
	$out .= "      <event-collection name='$name' events='$$evc{EVENTS}'/>\n";
      }
      $out .= "    </block>\n";
    }
    $out .= "  </processed-dataset>\n";
    $out .= "</dbs>\n";

    # Spit out the result
    $$dbs{DBH}->disconnect ();
    &reply_success ($out);
  };

  # If there was a failure, spit out an error.  Note that "exit" is a
  # special "die" call under mod_perl, so handle it gracefully.
  exit if $@ && ref $@ eq 'APR::Error' && $@ == ModPerl::EXIT;
  if ($@) { $@ =~ s/\n$//s; &reply_failure (402, "Execution error", $@) }
  eval { $$dbs{DBH}->disconnect () if $$dbs{DBH} };
}

######################################################################
# Get the files and blocks of a dataset.
sub getDatasetFiles
{
  # Parse parameters
  my @path = &getPath ();

  # Fetch the contents
  my $dbs;
  eval
  {
    # Connect to the database.
    $dbs = &connectToDBS ();

    # Translate the dataset path to an id
    my $id = &datasetFromPath ($$dbs{DBH}, @path);

    # Prepare output.
    my $out = "<?xml version='1.0' standalone='yes'?>\n<dbs>\n";
    my $pathname = "/$path[0]/$path[1]/$path[2]";
    $out .= "  <processed-dataset id='$id' path='$pathname'>\n";

    # Fetch blocks, event collections and files
    my ($blocks, $evcolls, $files) = &datasetContents ($$dbs{DBH}, $id);
    foreach my $block (@$blocks)
    {
      $out .= "    <block id='$$block{ID}' name='$pathname#$$block{ID}'";
      $out .= " files='$$block{TOTAL_FILES}' bytes='$$block{TOTAL_BYTES}'>\n";
      foreach my $file (values %{$$block{FILES}})
      {
	$out .= "      <file id='$$file{ID}' inblock='$$file{INBLOCK}'";
        $out .= " guid='$$file{GUID}' lfn='$$file{LOGICAL_NAME}'";
        $out .= " checksum='$$file{CHECKSUM}' size='$$file{FILESIZE}'";
        $out .= " status='@{[$$file{STATUS} || '']}' type='$$file{TYPE}'/>\n";
      }
      $out .= "    </block>\n";
    }
    $out .= "  </processed-dataset>\n";
    $out .= "</dbs>\n";

    # Spit out the result
    $$dbs{DBH}->disconnect ();
    &reply_success ($out);
  };

  # If there was a failure, spit out an error.  Note that "exit" is a
  # special "die" call under mod_perl, so handle it gracefully.
  exit if $@ && ref $@ eq 'APR::Error' && $@ == ModPerl::EXIT;
  if ($@) { $@ =~ s/\n$//s; &reply_failure (402, "Execution error", $@) }
  eval { $$dbs{DBH}->disconnect () if $$dbs{DBH} };
}

######################################################################
# Schema utilities.
sub datasetFromPath
{
  my ($dbs, $primary, $tier, $processed) = @_;
  my ($id) = &dbexec($$dbs{DBH}, qq{
    select procds.id
    from t_processed_dataset procds
      join t_primary_dataset primds
        on primds.id = procds.primary_dataset
      join t_processing_path ppath
        on ppath.id = procds.processing_path
      join t_data_tier dt
        on dt.id = ppath.data_tier
    where procds.name = :procname
      and primds.name = :primname
      and dt.name = :tiername},
    ":procname" => $processed,
    ":tiername" => $tier,
    ":primname" => $primary)
    ->fetchrow ();

  if (! defined $id)
  {
    $$dbs{DBH}->disconnect ();
    &reply_failure (302, "Bad dataset",
      "The dataset /$primary/$tier/$processed does not exist");
  }

  return $id;
}

# Obtain bulk of dataset contents in a convenient data structure.
sub datasetContents
{
  my ($dbs, $id) = @_;
  my $blocks = &dbexec ($$dbs{DBH}, qq{
    select id, status, files total_files, bytes total_bytes from t_block
    where processed_dataset = :id}, ":id" => $id)
    ->fetchall_arrayref({});

  my $evcolls = &dbexec ($$dbs{DBH}, qq{
    select
      evc.id,
      evc.collection_index,
      evi.events,
      evi.name
    from t_event_collection evc
      join t_info_evcoll evi on evi.event_collection = evc.id
    where evc.processed_dataset = :id}, ":id" => $id)
    ->fetchall_arrayref({});

  my $files = &dbexec ($$dbs{DBH}, qq{
    select
      evc.id evcoll,
      f.inblock,
      f.id,
      f.guid,
      f.logical_name,
      f.checksum,
      f.filesize,
      fs.name status,
      ft.name type
    from t_event_collection evc
      join t_evcoll_file evf
	on evf.evcoll = evc.id
      join t_file f
	on f.id = evf.fileid
      left join t_file_status fs
	on fs.id = f.status
      join t_file_type ft
	on ft.id = f.type
    where evc.processed_dataset = :id}, ":id" => $id)
    ->fetchall_arrayref({});

  # Link blocks, event collections and files to each other.
  my %id2block = map { $$_{ID} => $_ } @$blocks;
  my %id2evcoll = map { $$_{ID} => $_ } @$evcolls;
  foreach my $file (@$files)
  {
    my $block = $id2block{$$file{INBLOCK}};
    my $evcoll = $id2evcoll{$$file{EVCOLL}};
    push(@{$$evcoll{FILES}}, $file);
    $$block{FILES}{$$file{ID}} = $file;
    $$block{EVCS}{$$file{EVCOLL}} = $evcoll;
  }

  return ($blocks, $evcolls, $files);
}

# Simple tool to fetch everything from a table.  Useful for various
# mass fetch from various meta-data type tables.  Returns an array
# of hashes, where each hash has keys with the column names.
sub fetchAll
{
    my ($self, $kind) = @_;
    return $self->{uc($kind)}
        = &dbexec($self->{DBH}, qq{select * from t_$kind})->fetchall_arrayref ({});
}

######################################################################
######################################################################
######################################################################
###
### FIXME! IGNORE IGNORE IGNORE IGNORE IGNORE IGNORE IGNORE FIXME!
###
### The code below has been imported from PhEDEx UtilsDBS.  It does
### all the update logic, but hasn't yet been converted for use with
### the CGI server nor with the new schema.  You can safely ignore
### everything that follows, but it represents some portions of the
### the code that would need to be added.
###
######################################################################
######################################################################
sub makeNamed
{
    my ($self, $person, $kind, $name) = @_;
    return $self->getObject ($person, $kind, { NAME => $name });
}

sub makeMediator
{
    my ($self) = @_;
    return $self->{CACHED_MEDIATOR} if $self->{CACHED_MEDIATOR};

    my $user = getpwuid($<);
    my $host = &getfullhostname();
    my $app = $0; $app =~ s|.*/||;
    my $id = "host=$host#user=$user#app=$app";
    my $m = (grep($_->{NAME} eq $id, @{$self->{PERSON}}))[0];
    return $self->{CACHED_MEDIATOR} = $m if $m;

    $self->{CACHED_MEDIATOR} = $m = { NAME => $id,
	    			      CONTACT_INFO => "$user\@$host",
				      DISTINGUISHED_NAME => "/CN=$id" };
    return $self->newObject ($m, 'person', $m);
}

sub makePerson
{
    my ($self, $object) = @_;
    return $self->{CACHED_PERSON} if $self->{CACHED_PERSON};

    die "no ~/.globus/usercert.pem, cannot identify person\n"
        if ! -f "$ENV{HOME}/.globus/usercert.pem";

    my $email = scalar getpwuid($<) . '@' . &getfullhostname();
    my $certemail = qx(openssl x509 -in \$HOME/.globus/usercert.pem -noout -email 2>/dev/null);
    my $dn = qx(openssl x509 -in \$HOME/.globus/usercert.pem -noout -subject 2>/dev/null);
    my $name = (getpwuid($<))[6]; $name =~ s/,.*//;
    do { chomp($certemail); $email = $certemail }  if $certemail;
    do { chomp($dn); $dn =~ s/^subject\s*=\s*// } if $dn;
    do { $name = $1 } if ($dn && $dn =~ /CN=(.*?)( \d+)?(\/.*)?$/);

    my $p = (grep ($_->{NAME} eq $name, @{$self->{PERSON}}))[0];
    return $self->{CACHED_PERSON} = $p if $p;

    $self->{CACHED_PERSON} = $p = { NAME => $name,
				    CONTACT_INFO => $email,
				    DISTINGUISHED_NAME => $dn };
    return $self->newObject ($p, 'person', $p);
}

sub makeAppInfo
{
    my ($self, $context, $object, $person) = @_;
    my $appname = $object->{ApplicationName};
    my $appvers = $object->{ApplicationVersion};
    my $exe = $object->{ExecutableName};

    # Compute parameter set identification
    my $pset = join ("#", map { "$_=@{[md5_base64($object->{PARAMETERS}{$_})]}" }
    		          sort keys %{$object->{PARAMETERS}});

    my $appfamobj = $self->makeNamed ($person, "app_family", $appname);
    my $appobj = $self->getObject ($person, "application", {
	EXECUTABLE => $exe, APP_VERSION => $appvers, APP_FAMILY => $appfamobj->{ID} });
    my $appconfobj = $self->getObject ($person, "app_config", {
	APPLICATION => $appobj->{ID}, PARAMETER_SET => $pset });

    return $appconfobj;
}

sub makePrimaryDataset
{
    my ($self, $object, $person) = @_;
    my $primary = $self->getObject ($person, "primary_dataset", {
	NAME => $object->{DATASET} });
    return $primary;
}

sub makeProcessingPath
{
    my ($self, $object, $appinfo, $person) = @_;

    # If we have an input owner, use its processing path as a parent
    # to this one.  Otherwise start from null parent.
    my $parent = undef;
    if ($object->{DSINFO}{InputOwnerName})
    {
	($parent) = &dbexec($self->{DBH}, qq{
	    select procds.processing_path
	    from t_processed_dataset procds
	    join t_primary_dataset primds
	      on primds.id = procds.primary_dataset
	    where procds.name = :owner
	      and primds.name = :dataset},
            ":owner" => $object->{DSINFO}{InputOwnerName},
	    ":dataset" => $object->{DATASET})
    	    ->fetchrow();
    }

    my $tier = $self->makeNamed ($person, "data_tier", $object->{DSINFO}{DataTier});
    my $ppath = $self->getObject ($person, "processing_path", {
	PARENT => $parent,
	APP_CONFIG => $appinfo->{ID},
	DATA_TIER => $tier->{ID} });

    return $ppath;
}

sub findParentCollection
{
    my ($self, $object, $parent) = @_;
    my $qparent = $self->{STMTS}{QPARENT} ||= &dbprep ($self->{DBH}, qq{
	select ec.id
	from t_processed_dataset procds
	join t_primary_dataset primds
	  on primds.id = procds.primary_dataset
	join t_event_collection ec
	  on ec.processed_dataset = procds.id
	join t_info_evcoll evi
	  on evi.event_collection = ec.id
	where procds.name = :owner
          and primds.name = :dataset
          and evi.name = 'EvC_META'});

    &dbbindexec($qparent,
		":dataset" => $parent->{DATASET},
		":owner" => $parent->{OWNER});
    my ($id) = $qparent->fetchrow();
    $qparent->finish ();
    return $id;
}

# Prepare data for an array insert to a table.
sub prepInsert
{
    my ($self, $person, $sqlargs, $label, @args) = @_;
    my $sql = $self->{STMTS}{$label}{Statement};
    my ($kind) = ($sql =~ /insert into (\S+)/s);
    for (my $i = 0; $i < scalar @args; ++$i)
    {
	push(@{$sqlargs->{$label}{$i+1}}, $args[$i]);
    }

    push (@{$sqlargs->{IOBJHISTORY}{1}}, uc($kind));
    push (@{$sqlargs->{IOBJHISTORY}{2}}, $args[0]);
    push (@{$sqlargs->{IOBJHISTORY}{3}}, &mytimeofday());
    push (@{$sqlargs->{IOBJHISTORY}{4}}, $person->{ID});
    push (@{$sqlargs->{IOBJHISTORY}{5}}, $self->makeMediator ()->{ID});
}

# Update dataset information in database
sub updateDataset
{
    my ($self, $object) = @_;

    my %sqlargs = ();
    my $runs = $object->{RUNS};
    my $blocks = $object->{BLOCKS};
    my $files = $object->{FILES};

    # Initialise basic meta data (FIXME: take person etc. as input!)
    my $person = $self->makePerson ($object);
    my $appinfo = $self->makeAppInfo ($object, $object->{APPINFO}, $person);

    # Now go for the core data
    my $datatier = $self->makeNamed ($person, "data_tier", $object->{DSINFO}{DataTier});
    my $primary = $self->makePrimaryDataset ($object, $person);
    my $ppath = $self->makeProcessingPath ($object, $appinfo, $person);

    # Prepare statements
    my $dbh = $self->{DBH};
    my $stmtcache = $self->{STMTS} ||= {};
    $stmtcache->{IPROCDS} ||= &dbprep ($dbh, qq{
	insert into t_processed_dataset
	(id, primary_dataset, processing_path, name, is_open)
	values (?, ?, ?, ?, ?)});

    $stmtcache->{IEVCOLL} ||= &dbprep ($dbh, qq{
	insert into t_event_collection
	(id, processed_dataset, collection_index)
	values (?, ?, ?)});

    $stmtcache->{IEVCOLLINFO} ||= &dbprep ($dbh, qq{
	insert into t_info_evcoll
	(event_collection, events, name)
	values (?, ?, ?)});

    $stmtcache->{IBLOCK} ||= &dbprep ($dbh, qq{
	insert into t_block
	(id, processed_dataset, status, files, bytes)
	values (?, ?, ?, ?, ?)});

    $stmtcache->{IFILE} ||= &dbprep ($dbh, qq{
	insert into t_file
	(id, guid, logical_name, checksum, filesize, type, inblock)
	values (?, ?, ?, ?, ?, ?, ?)});

    $stmtcache->{IEVCOLLFILE} ||= &dbprep ($dbh, qq{
	insert into t_evcoll_file
	(id, evcoll, fileid)
	values (?, ?, ?)});

    $stmtcache->{IEVCOLLPROV} ||= &dbprep ($dbh, qq{
	insert into t_evcoll_parentage
	(id, parent, child, type)
	values (?, ?, ?, ?)});

    $stmtcache->{IOBJHISTORY} ||= &dbprep ($dbh, qq{
	insert into t_object_history
	(object_type, object_id, operation, at, person, mediator)
	values (?, ?, 'INSERT', ?, ?, ?)});

    # Insert dataset information
    $self->setID ("processed_dataset", $object);
    $self->prepInsert ($person, \%sqlargs, "IPROCDS",
	$object->{ID}, $primary->{ID}, $ppath->{ID}, $object->{OWNER},
	$object->{DSINFO}{CollectionStatus} eq 6 ? 'n' : 'y');

    foreach my $run ({ EVTS => 0, NAME => 0 }, values %$runs)
    {
        $self->setID ("event_collection", $run);
        $self->prepInsert ($person, \%sqlargs, "IEVCOLL",
	    $run->{ID}, $object->{ID}, $run->{NAME});

        $self->prepInsert ($person, \%sqlargs, "IEVCOLLINFO",
	    $run->{ID}, $run->{EVTS},
	    ($run->{NAME} == 0 ? "EvC_META" : "EvC_Run$run->{NAME}"));

	my %parentsdone = ();
	foreach my $parent (@{$object->{PARENTS}})
	{
	    # For the moment put dependencies only on META/META
	    next if $run->{NAME} != 0;

	    # For the moment, suppress duplicates -- the provenance from RefDB
	    # includes complete history, not just one level up, and as we map
	    # them all on "EvC_META", we can end up with duplicates here.
	    my $parentid = $self->findParentCollection ($object, $parent);
	    die "parent $parent->{OWNER}/$parent->{DATASET} of"
		. " $object->{OWNER}/$object->{DATASET} not found\n"
	        if ! defined $parentid;
	    next if $parentsdone{$parentid};
	    $parentsdone{$parentid} = 1;

	    my $x = $self->setID ("evcoll_parentage", {});
            $self->prepInsert ($person, \%sqlargs, "IEVCOLLPROV",
		$x->{ID}, $parentid, $run->{ID},
		$self->makeNamed ($person, "parentage_type", $parent->{TYPE})->{ID});
	}
    }

    foreach my $block (values %$blocks)
    {
	my $status = $object->{DSINFO}{CollectionStatus} eq 6 ? 'Closed' : 'Open';
	my $bytes = 0; map { $bytes += $_->{SIZE} || 0; $_ } values %{$block->{FILES}};
        $self->setID ("block", $block);
        $self->prepInsert ($person, \%sqlargs, "IBLOCK",
	    $block->{ID}, $object->{ID},
	    $self->makeNamed ($person, "block_status", $status)->{ID},
	    scalar values %{$block->{FILES}}, $bytes);
    }

    foreach my $file (values %$files)
    {
	$self->setID ("file", $file);
        $self->prepInsert ($person, \%sqlargs, "IFILE",
	    $file->{ID}, $file->{GUID}, $file->{LFN}[0],
	    $file->{CHECKSUM}, $file->{SIZE},
	    # $self->makeNamed ($person, "file_status", $status)->{ID},
	    $self->makeNamed ($person, "file_type", $file->{PFN}[0]{TYPE})->{ID},
	    $object->{BLOCKS}{$file->{INBLOCK}}{ID});

	foreach my $run (keys %{$file->{INRUNS}})
	{
	    my $x = $self->setID ("evcoll_file", {});
            $self->prepInsert ($person, \%sqlargs, "IEVCOLLFILE",
	        $x->{ID}, $object->{RUNS}{$run}{ID}, $file->{ID});
        }
    }

    # Grand execute everything
    foreach my $stmtname (qw(IPROCDS IEVCOLL IEVCOLLINFO IBLOCK IFILE
			     IEVCOLLFILE IEVCOLLPROV IOBJHISTORY))
    {
	next if ! keys %{$sqlargs{$stmtname}};
	my $stmt = $stmtcache->{$stmtname};
	foreach my $k (keys %{$sqlargs{$stmtname}}) {
	    $stmt->bind_param_array ($k, $sqlargs{$stmtname}{$k});
	}
	$stmt->execute_array ({ ArrayTupleResult => []});
    }

    # Now commit
    $dbh->commit();
}

# Find an existing cached object, or create a new one.  Takes an
# object template, and looks for identical cached object.  If one
# is found, it is returned.  Otherwise calls 'newObject' with the
# template and returns the result.
sub getObject
{
    my ($self, $person, $kind, $x) = @_;
    foreach my $obj (@{$self->{uc($kind)}})
    {
	return $obj
	    if ! grep(! exists $obj->{$_}
		      || ((defined $obj->{$_} ? $obj->{$_} : 'undef')
		          ne (defined $x->{$_} ? $x->{$_} : 'undef')),
		      keys %$x);
    }

    return $self->newObject ($person, $kind, $x);
}

sub newObject
{
    my ($self, $person, $kind, $object, @other) = @_;

    # If the object does not yet have an ID, allocate a new one,
    # create the object, and update our internal tables.
    if (! defined $object->{ID})
    {
	# Allocate new ID for this object
	($object->{ID}) = &dbexec ($self->{DBH},
	    qq{select seq_$kind.nextval from dual})->fetchrow();
	map { $object->{$_} = $object->{ID} } @other;

	# Execute SQL to create the object in the table
	my $createsql = "insert into t_$kind ("
	    . join (", ", map { lc($_) } sort keys %$object)
	    . ") values ("
	    . join (", ", map { ":" . lc($_) } sort keys %$object)
	    . ")";
	my %params = map { ":" . lc($_) => $object->{$_} } sort keys %$object;
	&dbexec ($self->{DBH}, $createsql, %params);

	# Update object history.  Note that if newObject was called by
	# makeMediator(), we produce a recursive call, but it all works
	# correctly because of the second time around it returns the
	# cached object, and we've already set the ID above on it.
	&dbexec ($self->{DBH}, qq{
	    insert into t_object_history
	    (object_type, object_id, operation, at, person, mediator)
	    values (:objtype, :objid, 'INSERT', :now, :person, :mediator)},
	    ":objtype" => uc("t_$kind"),
	    ":objid" => $object->{ID},
	    ":now" => &mytimeofday(),
	    ":person" => $person->{ID},
	    ":mediator" => $self->makeMediator()->{ID});

	# Memoize it
	push (@{$self->{uc($kind)}}, $object);
    }

    return $object;
}

sub setID
{
    my ($self, $kind, $object, @other) = @_;
    if (! defined $object->{ID})
    {
	($object->{ID}) = &dbexec ($self->{DBH},
	    qq{select seq_$kind.nextval from dual})->fetchrow();
	map { $object->{$_} = $object->{ID} } @other;
    }

    return $object;
}
